from flask import Flask, render_template, jsonify, request, redirect
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import hashlib
import logging
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse
import time
import re
from bs4 import BeautifulSoup
import urllib.parse
import json

@dataclass
class Article:
    """Data class for news articles with thumbnail support"""
    title: str
    content: str
    url: str
    published_date: datetime
    source: str
    thumbnail_url: Optional[str] = None
    country: Optional[str] = None
    hash_id: str = ""
    
    def __post_init__(self):
        content_hash = hashlib.md5(f"{self.title}{self.url}".encode()).hexdigest()
        self.hash_id = content_hash

class LGBTQNewsCollector:
    """Main news collection module for LGBTQ+ content with enhanced thumbnail extraction"""
    
    def __init__(self, newsapi_key: Optional[str] = None):
        self.newsapi_key = "eb080d6f006a4d068a852f914673d458"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LGBTQ-News-Agent/1.0'
        })
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Add thumbnail cache to avoid re-fetching same URLs
        self.thumbnail_cache = {}  # URL -> thumbnail_url mapping
        self.cache_max_age = 3600  # 1 hour cache
        
        # FIXED RSS feeds with proper Google News URLs
        self.rss_feeds = {
            'advocate': 'https://www.advocate.com/rss.xml',
            'pinknews': 'https://www.pinknews.co.uk/feed/',
            
            # Fixed Google News feeds 
            'google_lgbtq_atlanta': 'https://news.google.com/rss/search?q=gay%20atlanta%20-matt%20-jazz&hl=en-US&gl=US&ceid=US%3Aen',
            'google_lgbtq_general': 'https://news.google.com/rss/search?q=LGBTQ%20lesbian%20gay%20bisexual&hl=en-US&gl=US&ceid=US%3Aen',
            'google_gay_rights': 'https://news.google.com/rss/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNR1EyTTJ0MEVnSmxiaWdBUAE?hl=en-US&gl=US&ceid=US%3Aen',
            'google_pride_news': 'https://news.google.com/rss/search?q=pride%20month%20lgbtq&hl=en-US&gl=US&ceid=US%3Aen',
            'google_transgender_news': 'https://news.google.com/rss/search?q=transgender%20rights&hl=en-US&gl=US&ceid=US%3Aen',
            'atlanta_pride': 'https://atlantapride.org/feed/',
            # Standard RSS feeds
            'theguardian': 'https://www.theguardian.com/world/lgbt-rights/rss',
            'georgiavoice': 'https://rss.app/feed/CKNtVdCbbRqZov63',  
            'queerty': 'https://www.queerty.com/feed',
            'lgbtqnation': 'https://www.lgbtqnation.com/feed/',
            'washington_blade': 'https://www.washingtonblade.com/feed/',
            'outsports': 'https://www.outsports.com/rss/index.xml',
            'them': 'https://www.them.us/feed/rss',
            'gaycitynews': 'https://gaycitynews.com/feed/',
            'soundcloud': 'http://feeds.soundcloud.com/users/soundcloud:users:2640728/sounds.rss',
            'getoutspoken': 'https://getoutspoken.com/',
            'edge network': 'https://rss.app/feed/QvPRyrIsWcvNWNo5',
        }
        
        self.lgbtq_keywords = [
            'LGBTQ', 'LGBT', 'gay', 'lesbian', 'transgender', 'bisexual', 
            'queer', 'pride', 'rainbow', 'homosexual', 'same-sex', 
            'gender identity', 'sexual orientation', 'marriage equality',
            'trans rights', 'gay rights', 'GLAAD', 'Pride Month'
        ]
        
        # Default excluded keywords (can be overridden)
        self.default_excluded_keywords = [
            'death', 'died', 'suicide', 'murder', 'killed', 'violence',
            'attack', 'assault', 'harassment', 'abuse', 'hate crime'
        ]

    def normalize_datetime(self, dt: datetime) -> datetime:
        """Normalize datetime to timezone-aware UTC"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)

    def _fix_google_news_url(self, url: str) -> str:
        """Convert Google News web URLs to proper RSS format"""
        if 'news.google.com' not in url:
            return url
        
        # Fix search URLs
        if '/search?' in url and '/rss/' not in url:
            return url.replace('/search?', '/rss/search?')
        
        # Fix topic URLs  
        if '/topics/' in url and '/rss/' not in url:
            return url.replace('/topics/', '/rss/topics/')
        
        return url

    def _handle_google_news_feed(self, feed_url: str, source_name: str):
        """Special handling for Google News feeds with fallbacks"""
        try:
            # Fix the URL format first
            fixed_url = self._fix_google_news_url(feed_url)
            
            if fixed_url != feed_url:
                self.logger.info(f"Fixed Google News URL for {source_name}: {fixed_url}")
            
            # Try to fetch with specific headers that Google News likes
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0; +http://www.example.com/bot)',
                'Accept': 'application/rss+xml, application/xml, text/xml'
            }
            
            # Use requests to fetch with custom headers
            response = self.session.get(fixed_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # Parse the response
            feed = feedparser.parse(response.content)
            
            # Check if we got valid content
            if not feed.entries and hasattr(feed, 'bozo') and feed.bozo:
                self.logger.warning(f"Google News feed {source_name} returned no entries, trying alternative approach...")
                
                # Try the original URL as fallback
                fallback_feed = feedparser.parse(feed_url)
                if fallback_feed.entries:
                    return fallback_feed
                
                # If still no luck, skip this feed for this round
                self.logger.warning(f"Skipping {source_name} due to parsing issues")
                return feedparser.FeedParserDict()  # Return empty feed
            
            return feed
            
        except Exception as e:
            self.logger.error(f"Error handling Google News feed {source_name}: {e}")
            return feedparser.FeedParserDict()  # Return empty feed

    def _fetch_and_fix_feed(self, feed_url: str, source_name: str):
        """Enhanced feed fetcher with special handling for different sources"""
        try:
            # Special handling for Google News feeds
            if 'news.google.com' in feed_url:
                return self._handle_google_news_feed(feed_url, source_name)
            
            # Special handling for The Advocate feed (encoding issues)
            elif 'advocate.com' in feed_url:
                response = self.session.get(feed_url, timeout=15)
                response.raise_for_status()
                
                content = response.text
                content = content.replace('encoding="us-ascii"', 'encoding="utf-8"')
                content = content.replace("encoding='us-ascii'", "encoding='utf-8'")
                
                return feedparser.parse(content)
            
            # Standard handling for other feeds
            else:
                return feedparser.parse(feed_url)
                
        except Exception as e:
            self.logger.error(f"Error fetching feed {source_name}: {e}")
            return feedparser.FeedParserDict()  # Return empty feed

    def validate_feed_health(self) -> Dict[str, bool]:
        """Check which feeds are currently working"""
        feed_health = {}
        
        for source_name, feed_url in self.rss_feeds.items():
            try:
                self.logger.info(f"Testing feed: {source_name}")
                
                # Quick test fetch
                feed = self._fetch_and_fix_feed(feed_url, source_name)
                
                # Check if feed has entries and no major errors
                has_entries = bool(feed.entries)
                has_major_error = hasattr(feed, 'bozo') and feed.bozo and 'syntax error' in str(feed.bozo_exception)
                
                feed_health[source_name] = has_entries and not has_major_error
                
                if not feed_health[source_name]:
                    self.logger.warning(f"Feed {source_name} appears unhealthy: entries={has_entries}, error={has_major_error}")
                
            except Exception as e:
                self.logger.error(f"Feed {source_name} failed health check: {e}")
                feed_health[source_name] = False
        
        healthy_feeds = sum(feed_health.values())
        total_feeds = len(feed_health)
        self.logger.info(f"Feed health check complete: {healthy_feeds}/{total_feeds} feeds healthy")
        
        return feed_health

    def filter_articles_by_keywords(self, articles: List[Article], exclude_keywords: List[str] = None) -> List[Article]:
        """Filter out articles containing excluded keywords"""
        if not exclude_keywords:
            exclude_keywords = self.default_excluded_keywords
            
        if not exclude_keywords:
            return articles
        
        filtered_articles = []
        excluded_count = 0
        
        # Convert keywords to lowercase for case-insensitive matching
        exclude_keywords_lower = [kw.lower() for kw in exclude_keywords]
        
        for article in articles:
            # Check title and content for excluded keywords
            text_to_check = f"{article.title} {article.content}".lower()
            
            # Check if any excluded keyword appears in the article
            contains_excluded = any(keyword in text_to_check for keyword in exclude_keywords_lower)
            
            if not contains_excluded:
                filtered_articles.append(article)
            else:
                excluded_count += 1
                self.logger.debug(f"Excluded article: {article.title[:50]}...")
        
        if excluded_count > 0:
            self.logger.info(f"Filtered out {excluded_count} articles containing excluded keywords")
        
        return filtered_articles

    # ============= ENHANCED THUMBNAIL EXTRACTION METHODS =============

    def _get_cached_thumbnail(self, url: str) -> Optional[str]:
        """Get thumbnail from cache if available and not expired"""
        if url in self.thumbnail_cache:
            thumbnail_url, timestamp = self.thumbnail_cache[url]
            if time.time() - timestamp < self.cache_max_age:
                return thumbnail_url
            else:
                del self.thumbnail_cache[url]  # Remove expired entry
        return None

    def _cache_thumbnail(self, url: str, thumbnail_url: Optional[str]):
        """Cache thumbnail result"""
        self.thumbnail_cache[url] = (thumbnail_url, time.time())
        
        # Keep cache size reasonable
        if len(self.thumbnail_cache) > 1000:
            # Remove oldest entries
            items = list(self.thumbnail_cache.items())
            items.sort(key=lambda x: x[1][1])  # Sort by timestamp
            for old_url, _ in items[:100]:  # Remove oldest 100 entries
                del self.thumbnail_cache[old_url]

    def extract_thumbnail_from_rss(self, entry) -> Optional[str]:
        """Enhanced thumbnail image extraction from RSS entry"""
        
        # Priority 1: Check for media:thumbnail (most reliable)
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            for thumb in entry.media_thumbnail:
                url = thumb.get('url')
                if url and self._is_valid_image_url(url):
                    self.logger.debug(f"Found media:thumbnail: {url}")
                    return url
        
        # Priority 2: Check for media:content with image type
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image'):
                    url = media.get('url')
                    if url and self._is_valid_image_url(url):
                        self.logger.debug(f"Found media:content: {url}")
                        return url
        
        # Priority 3: Check for enclosures
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if enclosure.get('type', '').startswith('image'):
                    url = enclosure.get('href')
                    if url and self._is_valid_image_url(url):
                        self.logger.debug(f"Found enclosure: {url}")
                        return url
        
        # Priority 4: Parse content for images with enhanced patterns
        raw_content = self._get_raw_content(entry)
        if raw_content:
            thumbnail_url = self._extract_image_from_content(raw_content)
            if thumbnail_url:
                self.logger.debug(f"Found image in content: {thumbnail_url}")
                return thumbnail_url
        
        # Priority 5: Check entry links for image URLs
        if hasattr(entry, 'links'):
            for link in entry.links:
                if link.get('type', '').startswith('image'):
                    url = link.get('href')
                    if url and self._is_valid_image_url(url):
                        self.logger.debug(f"Found image link: {url}")
                        return url
        
        return None

    def _extract_image_from_content(self, content: str) -> Optional[str]:
        """Extract images from HTML content with comprehensive patterns"""
        if not content or len(content) > 100000:  # Skip very large content
            return None
        
        # Enhanced image extraction patterns (ordered by reliability)
        patterns = [
            # High quality patterns
            r'<img[^>]+class="[^"]*(?:featured|hero|main|primary|article|story)[^"]*"[^>]+src=[\'"]([^\'"]+)[\'"]',
            r'<img[^>]+src=[\'"]([^\'"]+)[\'"][^>]+class="[^"]*(?:featured|hero|main|primary|article|story)[^"]*"',
            
            # Open Graph and Twitter patterns in content
            r'<meta[^>]+property=[\'"]og:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]',
            r'<meta[^>]+content=[\'"]([^\'"]+)[\'"][^>]+property=[\'"]og:image[\'"]',
            r'<meta[^>]+name=[\'"]twitter:image[\'"][^>]+content=[\'"]([^\'"]+)[\'"]',
            
            # Figure and picture elements
            r'<figure[^>]*>.*?<img[^>]+src=[\'"]([^\'"]+)[\'"].*?</figure>',
            r'<picture[^>]*>.*?<img[^>]+src=[\'"]([^\'"]+)[\'"].*?</picture>',
            
            # Data attributes (lazy loading)
            r'<img[^>]+data-src=[\'"]([^\'"]+)[\'"]',
            r'<img[^>]+data-lazy-src=[\'"]([^\'"]+)[\'"]',
            r'<img[^>]+data-original=[\'"]([^\'"]+)[\'"]',
            
            # Srcset attributes
            r'<img[^>]+srcset=[\'"]([^\'"]+?\.(?:jpg|jpeg|png|webp|gif))[^\'";]+',
            
            # Background images
            r'background-image:\s*url\([\'"]?([^\'"();]+)[\'"]?\)',
            r'style=[\'"][^\'";]*background-image:\s*url\([\'"]?([^\'"();]+)[\'"]?\)',
            
            # General img tags (high quality)
            r'<img[^>]+src=[\'"]([^\'"]+\.(?:jpg|jpeg|png|webp|gif))[\'"][^>]*(?:width=[\'"](?:[3-9]\d\d+|[12]\d{3,})|height=[\'"](?:[3-9]\d\d+|[12]\d{3,}))',
            
            # Standard img tags
            r'<img[^>]+src=[\'"]([^\'"]+\.(?:jpg|jpeg|png|webp|gif))[\'"]',
            r'<img[^>]+src=[\'"]([^\'"]+)[\'"][^>]*>',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                url = match.group(1)
                if self._is_valid_image_url(url):
                    return url
        
        return None

    def _is_valid_image_url(self, url: str) -> bool:
        """Validate if URL appears to be a valid image"""
        if not url or len(url) < 10:
            return False
        
        # Skip data URLs, tracking pixels, and invalid formats
        invalid_indicators = [
            'data:', 'javascript:', 'mailto:',
            'pixel.gif', '1x1.', 'tracking', 'analytics',
            'spacer.gif', 'transparent.', 'blank.',
            'avatar-default', 'no-image', 'placeholder'
        ]
        
        url_lower = url.lower()
        if any(indicator in url_lower for indicator in invalid_indicators):
            return False
        
        # Must have valid image extension or be from known image hosts
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']
        known_image_hosts = [
            'imgur.com', 'cloudinary.com', 'wp.com', 'gravatar.com',
            'googleusercontent.com', 'amazonaws.com', 'cloudfront.net',
            'images.', 'img.', 'cdn.', 'static.', 'media.'
        ]
        
        has_extension = any(ext in url_lower for ext in image_extensions)
        has_image_host = any(host in url_lower for host in known_image_hosts)
        
        return has_extension or has_image_host

    def extract_thumbnail_from_url(self, url: str, max_retries: int = 2) -> Optional[str]:
        """Enhanced thumbnail extraction from article URL with retries and validation"""
        
        # Check cache first
        cached_result = self._get_cached_thumbnail(url)
        if cached_result is not None:
            return cached_result if cached_result != '' else None
        
        thumbnail_url = None
        
        for attempt in range(max_retries + 1):
            try:
                # Enhanced headers to avoid blocking
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                response = self.session.get(
                    url, 
                    headers=headers,
                    timeout=(8, 15),  # (connection, read) timeout
                    allow_redirects=True,
                    stream=False
                )
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Try multiple extraction methods in order of preference
                    thumbnail_url = (
                        self._extract_open_graph_image(soup, url) or
                        self._extract_twitter_card_image(soup, url) or
                        self._extract_structured_data_image(soup, url) or
                        self._extract_article_image(soup, url) or
                        self._extract_largest_image(soup, url)
                    )
                    
                    if thumbnail_url:
                        # Validate the extracted URL
                        if self._validate_image_url(thumbnail_url):
                            self.logger.debug(f"Successfully extracted thumbnail: {thumbnail_url}")
                            self._cache_thumbnail(url, thumbnail_url)
                            return thumbnail_url
                else:
                    self.logger.debug(f"HTTP {response.status_code} for {url}")
                    
            except (requests.exceptions.RequestException, Exception) as e:
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 2
                    self.logger.debug(f"Thumbnail extraction attempt {attempt + 1} failed for {url}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    self.logger.debug(f"Failed to extract thumbnail from {url}: {type(e).__name__}")
        
        # Cache negative result
        self._cache_thumbnail(url, '')
        return None

    def _extract_open_graph_image(self, soup, base_url: str) -> Optional[str]:
        """Extract Open Graph images"""
        og_selectors = [
            ('meta', {'property': 'og:image'}),
            ('meta', {'property': 'og:image:url'}),
            ('meta', {'property': 'og:image:secure_url'}),
            ('meta', {'name': 'og:image'}),
        ]
        
        for tag_name, attrs in og_selectors:
            meta_tag = soup.find(tag_name, attrs)
            if meta_tag and meta_tag.get('content'):
                img_url = meta_tag['content'].strip()
                if img_url and self._is_valid_image_url(img_url):
                    return urljoin(base_url, img_url)
        return None

    def _extract_twitter_card_image(self, soup, base_url: str) -> Optional[str]:
        """Extract Twitter Card images"""
        twitter_selectors = [
            ('meta', {'name': 'twitter:image'}),
            ('meta', {'name': 'twitter:image:src'}),
            ('meta', {'property': 'twitter:image'}),
            ('meta', {'name': 'twitter:image0'}),
        ]
        
        for tag_name, attrs in twitter_selectors:
            meta_tag = soup.find(tag_name, attrs)
            if meta_tag and meta_tag.get('content'):
                img_url = meta_tag['content'].strip()
                if img_url and self._is_valid_image_url(img_url):
                    return urljoin(base_url, img_url)
        return None

    def _extract_structured_data_image(self, soup, base_url: str) -> Optional[str]:
        """Extract images from JSON-LD structured data"""
        scripts = soup.find_all('script', type='application/ld+json')
        
        for script in scripts[:3]:  # Limit to first 3 for performance
            try:
                data = json.loads(script.string)
                
                # Handle both single objects and arrays
                items = data if isinstance(data, list) else [data]
                
                for item in items:
                    if isinstance(item, dict):
                        # Check various image properties
                        image_props = ['image', 'thumbnail', 'photo', 'logo']
                        
                        for prop in image_props:
                            if prop in item:
                                img_data = item[prop]
                                
                                # Handle different image data formats
                                if isinstance(img_data, str):
                                    if self._is_valid_image_url(img_data):
                                        return urljoin(base_url, img_data)
                                elif isinstance(img_data, list) and img_data:
                                    img_url = img_data[0]
                                    if isinstance(img_url, str) and self._is_valid_image_url(img_url):
                                        return urljoin(base_url, img_url)
                                    elif isinstance(img_url, dict) and 'url' in img_url:
                                        url = img_url['url']
                                        if self._is_valid_image_url(url):
                                            return urljoin(base_url, url)
                                elif isinstance(img_data, dict) and 'url' in img_data:
                                    url = img_data['url']
                                    if self._is_valid_image_url(url):
                                        return urljoin(base_url, url)
                                        
            except (json.JSONDecodeError, KeyError, AttributeError):
                continue
        
        return None

    def _extract_article_image(self, soup, base_url: str) -> Optional[str]:
        """Extract images from article content"""
        # Priority selectors for article images
        article_selectors = [
            'article img[src]',
            '.entry-content img[src]',
            '.post-content img[src]',
            '.article-body img[src]',
            '.story-body img[src]',
            'main img[src]',
            '[role="main"] img[src]',
            '.featured-image img[src]',
            '.hero-image img[src]',
        ]
        
        for selector in article_selectors:
            img_tags = soup.select(selector)
            for img_tag in img_tags:
                src = img_tag.get('src', '').strip()
                if src and self._is_valid_image_url(src):
                    return urljoin(base_url, src)
                
                # Check data attributes for lazy loading
                for attr in ['data-src', 'data-lazy-src', 'data-original']:
                    data_src = img_tag.get(attr, '').strip()
                    if data_src and self._is_valid_image_url(data_src):
                        return urljoin(base_url, data_src)
        
        return None

    def _extract_largest_image(self, soup, base_url: str) -> Optional[str]:
        """Find the largest/best quality image on the page"""
        all_images = soup.find_all('img')
        best_image = None
        best_score = 0
        
        for img in all_images:
            src = img.get('src', '').strip()
            if not src or not self._is_valid_image_url(src):
                continue
            
            # Score images based on various factors
            score = 0
            
            # Size-based scoring
            width = self._extract_number(img.get('width', ''))
            height = self._extract_number(img.get('height', ''))
            
            if width and height:
                score += min(width * height / 10000, 50)  # Cap at 50 points
            
            # Class-based scoring (important images usually have descriptive classes)
            img_class = img.get('class', [])
            if isinstance(img_class, list):
                img_class = ' '.join(img_class).lower()
            else:
                img_class = img_class.lower()
            
            important_classes = ['featured', 'hero', 'main', 'primary', 'article', 'story', 'thumbnail']
            score += sum(10 for cls in important_classes if cls in img_class)
            
            # Alt text scoring
            alt_text = img.get('alt', '').lower()
            if alt_text and len(alt_text) > 5:
                score += 5
            
            # Penalize small or tracking images
            if any(bad in src.lower() for bad in ['pixel', '1x1', 'spacer', 'tracking']):
                score -= 50
            
            if score > best_score:
                best_score = score
                best_image = src
        
        if best_image:
            return urljoin(base_url, best_image)
        
        return None

    def _extract_number(self, value: str) -> Optional[int]:
        """Extract numeric value from string"""
        if not value:
            return None
        try:
            return int(re.search(r'\d+', str(value)).group())
        except (AttributeError, ValueError):
            return None

    def _validate_image_url(self, url: str) -> bool:
        """Validate that an image URL is accessible and returns an image"""
        try:
            # Quick HEAD request to check if URL exists and is an image
            response = self.session.head(url, timeout=5, allow_redirects=True)
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                return content_type.startswith('image/')
            elif response.status_code == 405:  # Method not allowed, try GET
                response = self.session.get(url, timeout=5, stream=True)
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '').lower()
                    return content_type.startswith('image/')
                    
        except Exception:
            pass  # If validation fails, assume it's valid (better to try than miss)
        
        return True  # Default to True to avoid being too restrictive

    def _should_extract_from_url(self, source_name: str, url: str) -> bool:
        """Determine if we should attempt URL-based thumbnail extraction"""
        
        # Always try for high-quality sources
        priority_sources = [
            'advocate', 'queerty', 'lgbtqnation', 'washington_blade', 
            'theguardian', 'them', 'gaycitynews', 'outsports'
        ]
        
        if source_name in priority_sources:
            return True
        
        # Skip for problematic domains
        problematic_domains = [
            'soundcloud.com',  # Audio content
        ]
        
        for domain in problematic_domains:
            if domain in url:
                return False
        
        # For Google News, be more selective (they often have good images but can be slow)
        if source_name.startswith('google_'):
            return True  # Google News usually has good thumbnails
        
        # For PinkNews, try but with lower timeout (they have connection issues)
        if 'pinknews' in source_name:
            return True  # They have good images when accessible
        
        return True  # Default to trying

    # ============= END ENHANCED THUMBNAIL METHODS =============

    def _get_raw_content(self, entry) -> str:
        """Get raw HTML content without cleaning"""
        content_fields = ['content', 'summary', 'description']
        
        for field in content_fields:
            if hasattr(entry, field):
                content = getattr(entry, field)
                if isinstance(content, list) and content:
                    return content[0].get('value', '')
                elif isinstance(content, str):
                    return content
        
        return ""

    def collect_from_rss_with_resilience(self, hours_back: int = 24) -> List[Article]:
        """Collect articles from RSS feeds with enhanced thumbnail extraction"""
        articles = []
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
        failed_feeds = []
        successful_feeds = []
        thumbnail_stats = {'rss_success': 0, 'url_success': 0, 'total_attempts': 0}
        
        for source_name, feed_url in self.rss_feeds.items():
            try:
                self.logger.info(f"Fetching RSS feed: {source_name}")
                
                # Use enhanced feed fetcher
                feed = self._fetch_and_fix_feed(feed_url, source_name)
                
                if not feed.entries:
                    failed_feeds.append(source_name)
                    self.logger.warning(f"No entries found for {source_name}")
                    continue
                
                entry_count = 0
                for entry in feed.entries:
                    try:
                        pub_date = self._parse_date(entry)
                        
                        if pub_date:
                            normalized_pub_date = self.normalize_datetime(pub_date)
                            if normalized_pub_date < cutoff_time:
                                continue
                        
                        content = self._extract_content(entry)
                        thumbnail_stats['total_attempts'] += 1
                        
                        # Enhanced thumbnail extraction with better logging
                        thumbnail_url = None
                        
                        # First try RSS-embedded thumbnails
                        thumbnail_url = self.extract_thumbnail_from_rss(entry)
                        if thumbnail_url:
                            thumbnail_stats['rss_success'] += 1
                            self.logger.debug(f"RSS thumbnail found for {entry.title[:50]}: {thumbnail_url[:50]}...")
                        else:
                            # Fallback to URL extraction with smarter selection
                            if self._should_extract_from_url(source_name, entry.link):
                                thumbnail_url = self.extract_thumbnail_from_url(entry.link)
                                if thumbnail_url:
                                    thumbnail_stats['url_success'] += 1
                                    self.logger.debug(f"URL thumbnail found for {entry.title[:50]}: {thumbnail_url[:50]}...")
                        
                        final_pub_date = normalized_pub_date if pub_date else datetime.now(timezone.utc)
                        
                        article = Article(
                            title=entry.title,
                            content=content,
                            url=entry.link,
                            published_date=final_pub_date,
                            source=source_name,
                            thumbnail_url=thumbnail_url
                        )
                        
                        articles.append(article)
                        entry_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"Error processing entry from {source_name}: {e}")
                        continue
                
                if entry_count > 0:
                    successful_feeds.append(f"{source_name} ({entry_count} articles)")
                    self.logger.info(f"Successfully collected {entry_count} articles from {source_name}")
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                failed_feeds.append(source_name)
                self.logger.error(f"Failed to fetch RSS feed {source_name}: {e}")
                continue
        
        # Enhanced logging with thumbnail statistics
        total_articles = len(articles)
        articles_with_thumbs = len([a for a in articles if a.thumbnail_url])
        thumbnail_success_rate = (articles_with_thumbs / total_articles * 100) if total_articles > 0 else 0
        
        self.logger.info(f"RSS collection complete: {len(successful_feeds)} successful, {len(failed_feeds)} failed")
        self.logger.info(f"Thumbnail stats: {articles_with_thumbs}/{total_articles} articles with thumbnails ({thumbnail_success_rate:.1f}%)")
        self.logger.info(f"Thumbnail sources: RSS={thumbnail_stats['rss_success']}, URL={thumbnail_stats['url_success']}")
        
        if successful_feeds:
            self.logger.info(f"Successful feeds: {', '.join(successful_feeds)}")
        if failed_feeds:
            self.logger.warning(f"Failed feeds: {', '.join(failed_feeds)}")
        
        return articles

    def collect_from_newsapi(self, hours_back: int = 24) -> List[Article]:
        """Collect articles from NewsAPI with thumbnail support"""
        if not self.newsapi_key:
            self.logger.warning("NewsAPI key not provided, skipping NewsAPI collection")
            return []
        
        articles = []
        from_date = (datetime.now(timezone.utc) - timedelta(hours=hours_back)).strftime('%Y-%m-%d')
        
        for keyword in self.lgbtq_keywords[:5]:
            try:
                url = "https://newsapi.org/v2/everything"
                params = {
                    'q': keyword,
                    'from': from_date,
                    'sortBy': 'publishedAt',
                    'language': 'en',
                    'apiKey': self.newsapi_key,
                    'pageSize': 20
                }
                
                response = self.session.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                for article_data in data.get('articles', []):
                    try:
                        if not article_data.get('content') or article_data['content'] == '[Removed]':
                            continue
                        
                        pub_date = datetime.fromisoformat(
                            article_data['publishedAt'].replace('Z', '+00:00')
                        )
                        
                        thumbnail_url = article_data.get('urlToImage')
                        
                        article = Article(
                            title=article_data['title'],
                            content=article_data['description'] or article_data['content'],
                            url=article_data['url'],
                            published_date=pub_date,
                            source=article_data['source']['name'],
                            thumbnail_url=thumbnail_url
                        )
                        
                        articles.append(article)
                        
                    except Exception as e:
                        self.logger.error(f"Error processing NewsAPI article: {e}")
                        continue
                
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f"Error fetching from NewsAPI with keyword '{keyword}': {e}")
                continue
        
        return articles

    def collect_all_sources(self, hours_back: int = 24, exclude_keywords: List[str] = None) -> List[Article]:
        """Collect from all available sources with improved resilience"""
        all_articles = []
        
        rss_articles = self.collect_from_rss_with_resilience(hours_back)
        all_articles.extend(rss_articles)
        self.logger.info(f"Collected {len(rss_articles)} articles from RSS feeds")
        
        news_articles = self.collect_from_newsapi(hours_back)
        all_articles.extend(news_articles)
        self.logger.info(f"Collected {len(news_articles)} articles from NewsAPI")
        
        unique_articles = self._deduplicate_articles(all_articles)
        self.logger.info(f"After deduplication: {len(unique_articles)} unique articles")
        
        filtered_articles = self.filter_articles_by_keywords(unique_articles, exclude_keywords)
        self.logger.info(f"After keyword filtering: {len(filtered_articles)} articles")
        
        return filtered_articles

    def generate_google_news_url(self, query: str, language: str = 'en-US', country: str = 'US') -> str:
        """Generate a proper Google News RSS URL from a search query"""
        encoded_query = urllib.parse.quote_plus(query)
        return f'https://news.google.com/rss/search?q={encoded_query}&hl={language}&gl={country}&ceid={country}%3A{language.split("-")[0]}'

    def _parse_date(self, entry) -> Optional[datetime]:
        """Parse date from RSS entry with timezone handling"""
        date_fields = ['published_parsed', 'updated_parsed']
        
        for field in date_fields:
            if hasattr(entry, field) and getattr(entry, field):
                try:
                    time_struct = getattr(entry, field)
                    # Create timezone-aware datetime (assume UTC for RSS feeds)
                    dt = datetime(*time_struct[:6], tzinfo=timezone.utc)
                    return dt
                except:
                    continue
        
        date_strings = ['published', 'updated']
        for field in date_strings:
            if hasattr(entry, field):
                try:
                    from dateutil import parser
                    # Handle timezone parsing with EDT/EST mapping
                    tzinfos = {
                        'EDT': timezone(timedelta(hours=-4)),  # Eastern Daylight Time
                        'EST': timezone(timedelta(hours=-5)),  # Eastern Standard Time
                        'PDT': timezone(timedelta(hours=-7)),  # Pacific Daylight Time
                        'PST': timezone(timedelta(hours=-8)),  # Pacific Standard Time
                        'CDT': timezone(timedelta(hours=-5)),  # Central Daylight Time
                        'CST': timezone(timedelta(hours=-6)),  # Central Standard Time
                        'MDT': timezone(timedelta(hours=-6)),  # Mountain Daylight Time
                        'MST': timezone(timedelta(hours=-7)),  # Mountain Standard Time
                    }
                    dt = parser.parse(getattr(entry, field), tzinfos=tzinfos)
                    # Ensure timezone-aware
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except:
                    continue
        
        return None

    def _extract_content(self, entry) -> str:
        """Extract and clean content from RSS entry"""
        content_fields = ['content', 'summary', 'description']
        
        for field in content_fields:
            if hasattr(entry, field):
                content = getattr(entry, field)
                if isinstance(content, list) and content:
                    raw_content = content[0].get('value', '')
                elif isinstance(content, str):
                    raw_content = content
                else:
                    continue
                
                # Clean HTML tags and return clean text
                return self._clean_html_content(raw_content)
        
        return ""

    def _clean_html_content(self, html_content: str) -> str:
        """Remove HTML tags and clean up content"""
        if not html_content:
            return ""
        
        # Parse HTML and extract text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        
        # Clean up whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text

    def _deduplicate_articles(self, articles: List[Article]) -> List[Article]:
        """Remove duplicate articles based on hash_id"""
        seen_hashes = set()
        unique_articles = []
        
        for article in articles:
            if article.hash_id not in seen_hashes:
                seen_hashes.add(article.hash_id)
                unique_articles.append(article)
        
        return unique_articles

# Flask App
app = Flask(__name__)
collector = LGBTQNewsCollector()

@app.route('/')
def index():
    """Main page displaying recent LGBTQ+ news articles with aligned form"""
    hours_back = request.args.get('hours', 24, type=int)
    exclude_param = request.args.get('exclude', '')
    
    # Parse exclude keywords from comma-separated string
    exclude_keywords = [kw.strip() for kw in exclude_param.split(',') if kw.strip()] if exclude_param else None
    
    articles = collector.collect_all_sources(hours_back=hours_back, exclude_keywords=exclude_keywords)
    
    # Sort by publication date (newest first) with normalized datetimes
    articles.sort(key=lambda x: collector.normalize_datetime(x.published_date), reverse=True)
    
    # Create inline HTML template with aligned form
    articles_html = ""
    for article in articles:
        thumbnail_html = ""
        if article.thumbnail_url:
            # Change this line in the index() function:
            thumbnail_html = f'<img src="{article.thumbnail_url}" alt="Article thumbnail" style="width: 150px; height: 90px; object-fit: cover; border-radius: 5px; margin-right: 15px;">'
        
        articles_html += f'''
        <div style="display: flex; padding: 20px; margin: 15px 0; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); border-left: 4px solid #007bff;">
            {thumbnail_html}
            <div style="flex: 1;">
                <h3 style="margin: 0 0 8px 0; color: #495057;">
                    <a href="{article.url}" target="_blank" style="text-decoration: none; color: #007bff;">{article.title}</a>
                </h3>
                <p style="margin: 8px 0; color: #6c757d; line-height: 1.4;">{article.content[:200]}{'...' if len(article.content) > 200 else ''}</p>
                <div style="font-size: 12px; color: #868e96; margin-top: 10px;">
                    <span style="background: #e9ecef; padding: 2px 8px; border-radius: 12px; margin-right: 10px;">{article.source}</span>
                    <span>{article.published_date.strftime('%Y-%m-%d %H:%M UTC')}</span>
                </div>
            </div>
        </div>
        '''
    
    html_template = f'''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ðŸ§  The Brain - LGBTQ+ News Aggregator</title>
        <style>
            * {{
                box-sizing: border-box;
            }}
            body {{
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                margin: 0;
                padding: 20px;
                min-height: 100vh;
            }}
            .container {{
                max-width: 1000px;
                margin: 0 auto;
                background: #f8f9fa;
                border-radius: 15px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.2);
                overflow: hidden;
            }}
            .header {{
                background: #4169e1;  /* Solid Royal Blue - change this line only */
                padding: 30px;
                text-align: center;
                color: white;
            }}
            .header h1 {{
                margin: 0;
                font-size: 2.5em;
                font-weight: 300;
                text-shadow: 0 2px 4px rgba(0,0,0,0.3);
            }}
            .header p {{
                margin: 10px 0 0 0;
                opacity: 0.9;
                font-size: 1.1em;
            }}
            .controls {{
                padding: 25px;
                background: white;
                border-bottom: 1px solid #dee2e6;
            }}
            .form-row {{
                display: grid;
                grid-template-columns: 200px 1fr;
                gap: 15px;
                align-items: center;
                margin-bottom: 15px;
            }}
            .form-row:last-child {{
                margin-bottom: 0;
            }}
            .form-label {{
                font-weight: 600;
                color: #495057;
                text-align: right;
                padding-right: 10px;
            }}
            .form-input {{
                display: flex;
                gap: 10px;
                align-items: center;
            }}
            input[type="number"], input[type="text"] {{
                padding: 8px 12px;
                border: 2px solid #e9ecef;
                border-radius: 6px;
                font-size: 14px;
                transition: border-color 0.3s ease;
                flex: 1;
            }}
            input[type="number"]:focus, input[type="text"]:focus {{
                outline: none;
                border-color: #007bff;
                box-shadow: 0 0 0 3px rgba(0,123,255,0.1);
            }}
            .btn {{
                background: #007bff;
                color: white;
                border: none;
                padding: 8px 20px;
                border-radius: 6px;
                cursor: pointer;
                font-size: 14px;
                font-weight: 500;
                transition: background-color 0.3s ease;
                text-decoration: none;
                display: inline-block;
            }}
            .btn:hover {{
                background: #0056b3;
            }}
            .btn-secondary {{
                background: #6c757d;
                margin-left: 10px;
            }}
            .btn-secondary:hover {{
                background: #545b62;
            }}
            .stats {{
                padding: 20px 25px;
                background: #e8f4fd;
                border-bottom: 1px solid #bee5eb;
                text-align: center;
            }}
            .stats-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                gap: 20px;
                margin-top: 15px;
            }}
            .stat-item {{
                background: white;
                padding: 15px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            .stat-number {{
                font-size: 2em;
                font-weight: bold;
                color: #007bff;
                margin: 0;
            }}
            .stat-label {{
                font-size: 0.9em;
                color: #6c757d;
                margin: 5px 0 0 0;
            }}
            .articles {{
                padding: 25px;
                background: #f8f9fa;
            }}
            .no-articles {{
                text-align: center;
                padding: 40px;
                color: #6c757d;
            }}
            .form-help {{
                font-size: 12px;
                color: #6c757d;
                margin-top: 5px;
            }}
            @media (max-width: 768px) {{
                .form-row {{
                    grid-template-columns: 1fr;
                    text-align: left;
                }}
                .form-label {{
                    text-align: left;
                    padding-right: 0;
                    margin-bottom: 5px;
                }}
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>ðŸ§  The Brain</h1>
                <p>Your LGBTQ+ News Intelligence Hub</p>
            </div>
            
            <form method="GET" class="controls">
                <div class="form-row">
                    <label class="form-label">Hours back:</label>
                    <div class="form-input">
                        <input type="number" name="hours" value="{hours_back}" min="1" max="168" style="width: 100px;">
                        <button type="submit" class="btn">Update</button>
                        <a href="/refresh" class="btn btn-secondary">Refresh</a>
                        <a href="/admin" class="btn" style="background: #28a745;">Admin</a>
                    </div>
                </div>
                
                <div class="form-row">
                    <label class="form-label">Exclude keywords<br><span style="font-size: 11px; font-weight: normal;">(comma-separated):</span></label>
                    <div class="form-input">
                        <input type="text" name="exclude" value="{exclude_param}" 
                               placeholder="e.g., death, violence, murder">
                        <div class="form-help">
                            <strong>Default excluded:</strong> {','.join(collector.default_excluded_keywords)}<br>
                            <em>Leave blank to use defaults, or add your own terms</em>
                        </div>
                    </div>
                </div>
            </form>
            
            <div class="stats">
                <h3 style="margin: 0; color: #495057;">ðŸ“Š Current Collection Stats</h3>
                <div class="stats-grid">
                    <div class="stat-item">
                        <div class="stat-number">{len(articles)}</div>
                        <div class="stat-label">Articles Found</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{hours_back}</div>
                        <div class="stat-label">Hours Coverage</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{len(set(article.source for article in articles))}</div>
                        <div class="stat-label">News Sources</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{len([a for a in articles if a.thumbnail_url])}</div>
                        <div class="stat-label">With Images</div>
                    </div>
                </div>
            </div>
            
            <div class="articles">
                {"<div class='no-articles'><h3>ðŸ“° No articles found</h3><p>Try increasing the hours back or adjusting your keyword filters.</p></div>" if not articles else articles_html}
            </div>
        </div>
    </body>
    </html>
    '''
    
    return html_template

@app.route('/api/articles')
def api_articles():
    """API endpoint to get articles in JSON format"""
    hours_back = request.args.get('hours', 24, type=int)
    exclude_param = request.args.get('exclude', '')
    
    # Parse exclude keywords from comma-separated string
    exclude_keywords = [kw.strip() for kw in exclude_param.split(',') if kw.strip()] if exclude_param else None
    
    articles = collector.collect_all_sources(hours_back=hours_back, exclude_keywords=exclude_keywords)
    
    articles_data = []
    for article in articles:
        articles_data.append({
            'title': article.title,
            'content': article.content,
            'url': article.url,
            'published_date': article.published_date.isoformat(),
            'source': article.source,
            'thumbnail_url': article.thumbnail_url,
            'hash_id': article.hash_id
        })
    
    return jsonify({
        'articles': articles_data,
        'count': len(articles_data),
        'excluded_keywords': exclude_keywords or []
    })

@app.route('/refresh')
def refresh():
    """Force refresh of articles"""
    return redirect('/')

# Enhanced Admin Dashboard
@app.route('/admin')
def admin_dashboard():
    """Enhanced admin dashboard with improved JavaScript and error handling"""
    return '''
    <!DOCTYPE html>
    <html>
    <head>
        <title>The Brain - Admin Dashboard</title>
        <style>
            body { 
                font-family: Arial, sans-serif; 
                margin: 40px; 
                background-color: #f5f5f5; 
            }
            .container { 
                max-width: 1200px; 
                margin: 0 auto; 
                background: white; 
                padding: 30px; 
                border-radius: 10px; 
                box-shadow: 0 2px 10px rgba(0,0,0,0.1); 
            }
            .status-healthy { color: #28a745; font-weight: bold; }
            .status-unhealthy { color: #dc3545; font-weight: bold; }
            button { 
                padding: 12px 20px; 
                margin: 8px; 
                background: #007bff;
                color: white;
                border: none;
                border-radius: 5px;
                cursor: pointer;
                font-size: 14px;
                transition: background-color 0.3s;
            }
            button:hover { background: #0056b3; }
            button:disabled { 
                background: #6c757d; 
                cursor: not-allowed; 
            }
            .results { 
                margin-top: 20px; 
                padding: 20px; 
                background: #f8f9fa; 
                border-radius: 5px; 
                border-left: 4px solid #007bff; 
                min-height: 50px;
            }
            .feed-list { list-style: none; padding: 0; }
            .feed-item { 
                background: white; 
                margin: 10px 0; 
                padding: 15px; 
                border-radius: 5px; 
                border: 1px solid #dee2e6; 
            }
            .feed-url { color: #6c757d; font-size: 12px; word-break: break-all; }
            .header { color: #495057; border-bottom: 2px solid #007bff; padding-bottom: 10px; }
            .error { 
                background: #f8d7da; 
                color: #721c24; 
                padding: 15px; 
                border-radius: 5px; 
                margin: 10px 0;
                border-left: 4px solid #dc3545;
            }
            .loading {
                background: #d4edda;
                color: #155724;
                padding: 15px;
                border-radius: 5px;
                margin: 10px 0;
                border-left: 4px solid #28a745;
            }
            .debug {
                background: #e2e3e5;
                color: #383d41;
                padding: 10px;
                border-radius: 3px;
                font-family: monospace;
                font-size: 12px;
                margin-top: 10px;
                max-height: 200px;
                overflow-y: auto;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1 class="header">ðŸ§  The Brain - Feed Administration</h1>
            
            <h2>Feed Health Monitoring</h2>
            <p>Monitor and test your LGBTQ+ news feed sources for "The Brain" project.</p>
            
            <div style="margin: 20px 0;">
                <button id="healthBtn" onclick="checkFeedHealth()">ðŸ” Check All Feeds Health</button>
                <button id="listBtn" onclick="listFeeds()">ðŸ“ List All Feeds</button>
                <button id="statsBtn" onclick="showStats()">ðŸ“Š Show Statistics</button>
                <button onclick="goHome()" style="background: #28a745;">ðŸ  Back to News</button>
                <button onclick="toggleDebug()" style="background: #6c757d;" id="debugBtn">ðŸ› Debug</button>
            </div>
            
            <div id="results" class="results" style="display: none;">
                <div id="content">Ready to check feeds...</div>
                <div id="debug" class="debug" style="display: none;">
                    <strong>Debug Log:</strong><br>
                    <div id="debugContent">Click 'Debug' to show debug information</div>
                </div>
            </div>
        </div>
        
        <script>
            let debugMode = false;
            let debugLog = [];
            
            function log(message) {
                console.log(message);
                debugLog.push(new Date().toLocaleTimeString() + ': ' + message);
                if (debugMode) {
                    updateDebugDisplay();
                }
            }
            
            function updateDebugDisplay() {
                const debugContent = document.getElementById('debugContent');
                if (debugContent) {
                    debugContent.innerHTML = debugLog.slice(-20).join('<br>'); // Show last 20 entries
                }
            }
            
            function toggleDebug() {
                debugMode = !debugMode;
                const debugDiv = document.getElementById('debug');
                const debugBtn = document.getElementById('debugBtn');
                
                if (debugMode) {
                    debugDiv.style.display = 'block';
                    debugBtn.textContent = 'ðŸ› Hide Debug';
                    updateDebugDisplay();
                } else {
                    debugDiv.style.display = 'none';
                    debugBtn.textContent = 'ðŸ› Debug';
                }
                log('Debug mode ' + (debugMode ? 'enabled' : 'disabled'));
            }
            
            function showResults() {
                document.getElementById('results').style.display = 'block';
            }
            
            function setLoading(message) {
                showResults();
                document.getElementById('content').innerHTML = 
                    '<div class="loading">ðŸ”„ ' + message + '</div>';
                log('Loading: ' + message);
            }
            
            function setError(message, details = '') {
                showResults();
                document.getElementById('content').innerHTML = 
                    '<div class="error">âŒ ' + message + 
                    (details ? '<br><small>' + details + '</small>' : '') + '</div>';
                log('Error: ' + message + (details ? ' - ' + details : ''));
            }
            
            function disableButton(buttonId) {
                const btn = document.getElementById(buttonId);
                if (btn) {
                    btn.disabled = true;
                    setTimeout(() => { btn.disabled = false; }, 3000);
                }
            }
            
            function goHome() {
                log('Navigating to home page');
                window.location.href = '/';
            }
            
            function checkFeedHealth() {
                log('Starting feed health check');
                setLoading('Checking feed health... This may take 30-60 seconds...');
                disableButton('healthBtn');
                
                fetch('/admin/feed-health')
                    .then(response => {
                        log('Feed health response status: ' + response.status);
                        if (!response.ok) {
                            throw new Error('HTTP ' + response.status + ': ' + response.statusText);
                        }
                        return response.json();
                    })
                    .then(data => {
                        log('Feed health data received');
                        
                        let healthyList = data.healthy_feed_list && data.healthy_feed_list.length > 0 
                            ? data.healthy_feed_list.join(', ') 
                            : 'None';
                        let unhealthyList = data.unhealthy_feed_list && data.unhealthy_feed_list.length > 0 
                            ? data.unhealthy_feed_list.join(', ') 
                            : 'None';
                            
                        document.getElementById('content').innerHTML = `
                            <h3>ðŸ“Š Feed Health Report</h3>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                                <div style="text-align: center; padding: 20px; background: #d4edda; border-radius: 10px;">
                                    <h4 style="margin: 0; color: #155724;">âœ… Healthy Feeds</h4>
                                    <div style="font-size: 2em; color: #28a745; margin: 10px 0;">${data.healthy_feeds || 0}</div>
                                </div>
                                <div style="text-align: center; padding: 20px; background: #f8d7da; border-radius: 10px;">
                                    <h4 style="margin: 0; color: #721c24;">âŒ Unhealthy Feeds</h4>
                                    <div style="font-size: 2em; color: #dc3545; margin: 10px 0;">${data.unhealthy_feeds || 0}</div>
                                </div>
                            </div>
                            <p><strong>Total Feeds:</strong> ${data.total_feeds || 'Unknown'}</p>
                            <p><strong>Healthy Feeds:</strong> <span class="status-healthy">${healthyList}</span></p>
                            <p><strong>Unhealthy Feeds:</strong> <span class="status-unhealthy">${unhealthyList}</span></p>
                            <p><small>Last checked: ${data.timestamp ? new Date(data.timestamp).toLocaleString() : 'Unknown'}</small></p>
                        `;
                        log('Feed health check completed successfully');
                    })
                    .catch(error => {
                        setError('Failed to check feed health', error.message);
                        log('Feed health check failed: ' + error.message);
                    });
            }
            
            function listFeeds() {
                log('Starting feed list');
                setLoading('Loading feed list...');
                disableButton('listBtn');
                
                fetch('/admin/feeds')
                    .then(response => {
                        log('Feed list response status: ' + response.status);
                        if (!response.ok) {
                            throw new Error('HTTP ' + response.status + ': ' + response.statusText);
                        }
                        return response.json();
                    })
                    .then(data => {
                        log('Feed list data received');
                        
                        let html = '<h3>ðŸ“° All Configured News Feeds (' + (data.feeds ? data.feeds.length : 0) + ')</h3>';
                        
                        if (!data.feeds || data.feeds.length === 0) {
                            html += '<p>No feeds configured.</p>';
                        } else {
                            html += '<ul class="feed-list">';
                            data.feeds.forEach(feed => {
                                let typeIcon = feed.type === 'google_news' ? 'ðŸ”' : 'ðŸ“¡';
                                html += `
                                    <li class="feed-item">
                                        <div style="display: flex; justify-content: space-between; align-items: center;">
                                            <div>
                                                <strong>${typeIcon} ${feed.name}</strong> 
                                                <span style="background: #e9ecef; padding: 2px 8px; border-radius: 12px; font-size: 11px; margin-left: 10px;">${feed.type}</span>
                                                <div class="feed-url">${feed.url}</div>
                                            </div>
                                            <button onclick="testFeed('${feed.name}')" style="margin: 0;">ðŸ§ª Test</button>
                                        </div>
                                    </li>
                                `;
                            });
                            html += '</ul>';
                        }
                        
                        document.getElementById('content').innerHTML = html;
                        log('Feed list loaded successfully');
                    })
                    .catch(error => {
                        setError('Failed to load feed list', error.message);
                        log('Feed list failed: ' + error.message);
                    });
            }
            
            function testFeed(feedName) {
                log('Testing feed: ' + feedName);
                if (!feedName) {
                    setError('Invalid feed name');
                    return;
                }
                
                if (confirm('Test feed: ' + feedName + '? This may take 10-30 seconds.')) {
                    setLoading('Testing feed: ' + feedName + '...');
                    
                    fetch('/admin/test-feed/' + encodeURIComponent(feedName))
                        .then(response => {
                            log('Test feed response status for ' + feedName + ': ' + response.status);
                            return response.json();
                        })
                        .then(data => {
                            log('Test feed data for ' + feedName);
                            
                            let status = data.error ? 'âŒ FAILED' : 'âœ… SUCCESS';
                            let details = data.error 
                                ? `Error: ${data.error}`
                                : `Found ${data.entries_found || 0} entries\\nFetch time: ${data.fetch_time_seconds || 'N/A'}s\\nFeed title: ${data.feed_title || 'Unknown'}`;
                            
                            alert(status + '\\n\\nFeed: ' + feedName + '\\n\\n' + details);
                            log('Test feed completed for ' + feedName);
                        })
                        .catch(error => {
                            setError('Test failed for feed: ' + feedName, error.message);
                            log('Test feed failed for ' + feedName + ': ' + error.message);
                        });
                }
            }
            
            function showStats() {
                log('Showing statistics');
                showResults();
                
                document.getElementById('content').innerHTML = `
                    <h3>ðŸ“ˆ The Brain Statistics</h3>
                    <div style="display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: center;">
                        <div><strong>ðŸŽ¯ Project:</strong></div><div>LGBTQ+ News Aggregator</div>
                        <div><strong>ðŸ”§ Status:</strong></div><div>Enhanced Production Ready</div>
                        <div><strong>ðŸ“¡ Sources:</strong></div><div>15 RSS feeds + NewsAPI</div>
                        <div><strong>ðŸ·ï¸ Keywords:</strong></div><div>17 LGBTQ+ focused terms</div>
                        <div><strong>ðŸš« Excluded:</strong></div><div>7 negative keywords filtered</div>
                        <div><strong>âš¡ Features:</strong></div><div>Enhanced thumbnail extraction, caching, deduplication, timezone handling</div>
                        <div><strong>ðŸ–¼ï¸ Thumbnails:</strong></div><div>Multi-source extraction (RSS + URL scraping)</div>
                        <div><strong>ðŸ›¡ï¸ Resilience:</strong></div><div>Advanced error handling, retry logic, feed health monitoring</div>
                    </div>
                    <div style="margin-top: 20px; padding: 15px; background: #e8f4fd; border-radius: 5px;">
                        <strong>ðŸ”— Quick Links:</strong><br>
                        <a href="/" style="color: #007bff; text-decoration: none; margin-right: 20px;">ðŸ“° Main News Page</a>
                        <a href="/api/articles" style="color: #007bff; text-decoration: none; margin-right: 20px;">ðŸ“‹ API Endpoint</a>
                        <a href="/admin/feed-health" style="color: #007bff; text-decoration: none;">ðŸ”§ Feed Health JSON</a>
                    </div>
                `;
                log('Statistics displayed');
            }
            
            // Initialize page
            document.addEventListener('DOMContentLoaded', function() {
                log('Admin dashboard loaded');
                log('JavaScript initialized successfully');
            });
            
            // Test basic functionality on load
            log('Admin dashboard script loaded successfully');
        </script>
    </body>
    </html>
    '''

@app.route('/admin/feed-health')
def admin_feed_health():
    """API endpoint to check feed health status"""
    feed_health = collector.validate_feed_health()
    
    healthy_feeds = [name for name, status in feed_health.items() if status]
    unhealthy_feeds = [name for name, status in feed_health.items() if not status]
    
    return jsonify({
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'total_feeds': len(feed_health),
        'healthy_feeds': len(healthy_feeds),
        'unhealthy_feeds': len(unhealthy_feeds),
        'healthy_feed_list': healthy_feeds,
        'unhealthy_feed_list': unhealthy_feeds,
        'feed_details': feed_health
    })

@app.route('/admin/feeds')
def admin_list_feeds():
    """API endpoint to list all configured feeds"""
    return jsonify({
        'feeds': [
            {
                'name': name,
                'url': url,
                'type': 'google_news' if 'news.google.com' in url else 'standard_rss'
            }
            for name, url in collector.rss_feeds.items()
        ]
    })

@app.route('/admin/test-feed/<feed_name>')
def admin_test_single_feed(feed_name):
    """Test a specific feed and return detailed results"""
    if feed_name not in collector.rss_feeds:
        return jsonify({'error': 'Feed not found'}), 404
    
    feed_url = collector.rss_feeds[feed_name]
    
    try:
        start_time = time.time()
        
        feed = collector._fetch_and_fix_feed(feed_url, feed_name)
        
        fetch_time = time.time() - start_time
        
        return jsonify({
            'feed_name': feed_name,
            'feed_url': feed_url,
            'fetch_time_seconds': round(fetch_time, 2),
            'entries_found': len(feed.entries) if feed.entries else 0,
            'feed_title': getattr(feed.feed, 'title', 'Unknown') if hasattr(feed, 'feed') else 'Unknown',
            'last_updated': getattr(feed.feed, 'updated', 'Unknown') if hasattr(feed, 'feed') else 'Unknown',
            'has_bozo_error': hasattr(feed, 'bozo') and feed.bozo,
            'bozo_exception': str(feed.bozo_exception) if hasattr(feed, 'bozo_exception') else None,
            'sample_titles': [entry.title for entry in feed.entries[:3]] if feed.entries else []
        })
        
    except Exception as e:
        return jsonify({
            'feed_name': feed_name,
            'feed_url': feed_url,
            'error': str(e),
            'status': 'failed'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)