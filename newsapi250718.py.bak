from flask import Flask, render_template, jsonify, request, redirect
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import hashlib
import logging
from dataclasses import dataclass
from urllib.parse import urljoin
import time
import re
from bs4 import BeautifulSoup
from dateutil import parser

@dataclass
class Article:
    """Data class for news articles with thumbnail support"""
    title: str
    content: str
    url: str
    published_date: datetime
    source: str
    thumbnail_url: Optional[str] = None
    country: Optional[str]    = None
    hash_id: str               = ""

    def __post_init__(self):
        # If a naive datetime crept in, assume UTC
        if self.published_date.tzinfo is None:
            self.published_date = self.published_date.replace(tzinfo=timezone.utc)

        # Generate stable hash ID
        content_hash = hashlib.md5(f"{self.title}{self.url}".encode()).hexdigest()
        self.hash_id = content_hash

class LGBTQNewsCollector:
    """Main news collection module for LGBTQ+ content with thumbnail extraction"""

    def __init__(self, newsapi_key: Optional[str] = None):
        self.newsapi_key = newsapi_key or "eb080d6f006a4d068a852f914673d458"
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'LGBTQ-News-Agent/1.0'})

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        self.rss_feeds = {
            'advocate':        'https://www.advocate.com/rss.xml',
            'pinknews':        'https://www.pinknews.co.uk/feed/',
            'queerty':         'https://www.queerty.com/feed',
            'lgbtqnation':     'https://www.lgbtqnation.com/feed/',
            'washington_blade':'https://www.washingtonblade.com/feed/',
            'outsports':       'https://www.outsports.com/rss/index.xml',
            'them':            'https://www.them.us/feed/rss',
            'gaycitynews':     'https://gaycitynews.com/feed/',
        }

        self.lgbtq_keywords = [
            'LGBTQ', 'LGBT', 'gay', 'lesbian', 'transgender',
            'bisexual', 'queer', 'pride', 'rainbow', 'homosexual',
            'same-sex', 'gender identity', 'sexual orientation',
            'marriage equality', 'trans rights', 'gay rights',
            'GLAAD', 'Pride Month'
        ]

    def _parse_date(self, entry) -> Optional[datetime]:
        """Parse date from RSS entry and return a UTC‐aware datetime"""
        # Structured time fields
        for field in ('published_parsed', 'updated_parsed'):
            if hasattr(entry, field) and getattr(entry, field):
                try:
                    ts = getattr(entry, field)
                    dt = datetime(*ts[:6], tzinfo=timezone.utc)
                    return dt
                except Exception:
                    pass

        # String‐based fallback
        for field in ('published', 'updated'):
            if hasattr(entry, field):
                try:
                    dt = parser.parse(getattr(entry, field))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except Exception:
                    pass

        return None

    def _extract_content(self, entry) -> str:
        """Extract the main HTML/text content from an RSS entry"""
        for field in ('content', 'summary', 'description'):
            if hasattr(entry, field):
                val = getattr(entry, field)
                if isinstance(val, list) and val:
                    return val[0].get('value', '')
                elif isinstance(val, str):
                    return val
        return ""

    def extract_thumbnail_from_rss(self, entry) -> Optional[str]:
        """Pull an image URL out of RSS media tags or embedded HTML"""
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url')
        if hasattr(entry, 'media_content'):
            for m in entry.media_content:
                if m.get('type', '').startswith('image'):
                    return m.get('url')
        if hasattr(entry, 'enclosures'):
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image'):
                    return enc.get('href')
        # Try in-text <img> tags
        content = self._extract_content(entry)
        if content:
            match = re.search(r'<img[^>]+src=[\'"]([^\'"]+)[\'"]', content)
            if match:
                return match.group(1)
        return None

    def extract_thumbnail_from_url(self, url: str) -> Optional[str]:
        """Fetch the article and scrape Open Graph / Twitter card images"""
        try:
            resp = self.session.get(url, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, 'html.parser')

            # Open Graph
            og = soup.find('meta', property='og:image')
            if og and og.get('content'):
                return og['content']

            # Twitter card
            tw = soup.find('meta', attrs={'name': 'twitter:image'})
            if tw and tw.get('content'):
                return tw['content']

            # First <img>
            img = soup.find('img')
            if img and img.get('src'):
                return urljoin(url, img['src'])

        except Exception as e:
            self.logger.warning(f"Thumbnail scrape failed for {url}: {e}")
        return None

    def collect_from_rss(self, hours_back: int = 24) -> List[Article]:
        """Pull RSS entries, extract thumbnails, and wrap in Article objects"""
        articles = []
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours_back)

        for src, feed_url in self.rss_feeds.items():
            self.logger.info(f"Fetching RSS feed: {src}")
            feed = feedparser.parse(feed_url)
            if feed.bozo:
                self.logger.warning(f"Parse warning for {src}: {feed.bozo_exception}")

            for entry in feed.entries:
                pub = self._parse_date(entry) or datetime.now(timezone.utc)
                if pub < cutoff:
                    continue

                content       = self._extract_content(entry)
                thumb         = self.extract_thumbnail_from_rss(entry) \
                              or self.extract_thumbnail_from_url(entry.link)

                try:
                    articles.append(Article(
                        title          = entry.title,
                        content        = content,
                        url            = entry.link,
                        published_date = pub,
                        source         = src,
                        thumbnail_url  = thumb
                    ))
                except Exception as e:
                    self.logger.error(f"Error creating Article from {src}: {e}")

            time.sleep(1)

        return articles

    def collect_from_newsapi(self, hours_back: int = 24) -> List[Article]:
        """Fetch NewsAPI articles, keep only those with content, and apply thumbnails"""
        if not self.newsapi_key:
            self.logger.warning("No NewsAPI key; skipping")
            return []

        articles = []
        from_date = (datetime.now(timezone.utc) - timedelta(hours=hours_back))\
                    .strftime('%Y-%m-%d')

        for kw in self.lgbtq_keywords[:5]:
            try:
                params = {
                    'q'       : kw,
                    'from'    : from_date,
                    'sortBy'  : 'publishedAt',
                    'language': 'en',
                    'apiKey'  : self.newsapi_key,
                    'pageSize': 20
                }
                resp = self.session.get("https://newsapi.org/v2/everything",
                                         params=params, timeout=10)
                resp.raise_for_status()
                data = resp.json()

                for art in data.get('articles', []):
                    if not art.get('content') or art['content'] == '[Removed]':
                        continue

                    # ISO8601 → aware datetime
                    dt = datetime.fromisoformat(
                        art['publishedAt'].replace('Z', '+00:00')
                    )
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)

                    articles.append(Article(
                        title          = art['title'],
                        content        = art.get('description') or art['content'],
                        url            = art['url'],
                        published_date = dt,
                        source         = art['source']['name'],
                        thumbnail_url  = art.get('urlToImage')
                    ))

                time.sleep(2)

            except Exception as e:
                self.logger.error(f"NewsAPI fetch error for '{kw}': {e}")

        return articles

    def _deduplicate_articles(self, articles: List[Article]) -> List[Article]:
        """Drop duplicates by hash_id"""
        seen, unique = set(), []
        for art in articles:
            if art.hash_id not in seen:
                seen.add(art.hash_id)
                unique.append(art)
        return unique

    def collect_all_sources(self, hours_back: int = 24) -> List[Article]:
        """Combine RSS + NewsAPI, dedupe, and return a unified list"""
        rss_list  = self.collect_from_rss(hours_back)
        api_list  = self.collect_from_newsapi(hours_back)
        self.logger.info(f"RSS: {len(rss_list)}, NewsAPI: {len(api_list)}")

        combined  = rss_list + api_list
        unique    = self._deduplicate_articles(combined)
        self.logger.info(f"After dedupe: {len(unique)} unique articles")
        return unique

# Flask app setup
app = Flask(__name__)
collector = LGBTQNewsCollector()

@app.route('/')
def index():
    """Render homepage with sorted articles"""
    hours_back = request.args.get('hours', 24, type=int)
    articles   = collector.collect_all_sources(hours_back)

    # All dates are UTC‐aware by now, so sort works without error
    articles.sort(key=lambda x: x.published_date, reverse=True)
    return render_template('news.html', articles=articles, hours_back=hours_back)

@app.route('/api/articles')
def api_articles():
    """JSON endpoint"""
    hours_back    = request.args.get('hours', 24, type=int)
    article_objs  = collector.collect_all_sources(hours_back)
    article_objs.sort(key=lambda x: x.published_date, reverse=True)

    return jsonify({
        'count'   : len(article_objs),
        'articles': [{
            'title'         : art.title,
            'content'       : art.content,
            'url'           : art.url,
            'published_date': art.published_date.isoformat(),
            'source'        : art.source,
            'thumbnail_url' : art.thumbnail_url,
            'hash_id'       : art.hash_id
        } for art in article_objs]
    })

@app.route('/refresh')
def refresh():
    """Redirect to force data refresh"""
    return redirect('/')

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)