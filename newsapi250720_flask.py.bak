from flask import Flask, render_template, jsonify, request, redirect
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import hashlib
import logging
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse
import time
import re
from bs4 import BeautifulSoup
import urllib.parse

@dataclass
class Article:
    """Data class for news articles with thumbnail support"""
    title: str
    content: str
    url: str
    published_date: datetime
    source: str
    thumbnail_url: Optional[str] = None
    country: Optional[str] = None
    hash_id: str = ""
    
    def __post_init__(self):
        content_hash = hashlib.md5(f"{self.title}{self.url}".encode()).hexdigest()
        self.hash_id = content_hash

class LGBTQNewsCollector:
    """Main news collection module for LGBTQ+ content with thumbnail extraction"""
    
    def __init__(self, newsapi_key: Optional[str] = None):
        self.newsapi_key = "eb080d6f006a4d068a852f914673d458"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LGBTQ-News-Agent/1.0'
        })
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # FIXED RSS feeds with proper Google News URLs
        self.rss_feeds = {
            'advocate': 'https://www.advocate.com/rss.xml',
            'pinknews': 'https://www.pinknews.co.uk/feed/',
            
            # Fixed Google News feeds 
            'google_lgbtq_atlanta': 'https://news.google.com/rss/search?q=gay%20atlanta%20-matt%20-jazz&hl=en-US&gl=US&ceid=US%3Aen',
            'google_lgbtq_general': 'https://news.google.com/rss/search?q=LGBTQ%20lesbian%20gay%20bisexual&hl=en-US&gl=US&ceid=US%3Aen',  # FIXED
            'google_gay_rights': 'https://news.google.com/rss/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNR1EyTTJ0MEVnSmxiaWdBUAE?hl=en-US&gl=US&ceid=US%3Aen',
            'google_pride_news': 'https://news.google.com/rss/search?q=pride%20month%20lgbtq&hl=en-US&gl=US&ceid=US%3Aen',
            'google_transgender_news': 'https://news.google.com/rss/search?q=transgender%20rights&hl=en-US&gl=US&ceid=US%3Aen',
            
            'theguardian': 'https://www.theguardian.com/world/lgbt-rights/rss',
            'queerty': 'https://www.queerty.com/feed',
            'lgbtqnation': 'https://www.lgbtqnation.com/feed/',
            'washington_blade': 'https://www.washingtonblade.com/feed/',
            'outsports': 'https://www.outsports.com/rss/index.xml',
            'them': 'https://www.them.us/feed/rss',
            'gaycitynews': 'https://gaycitynews.com/feed/',
            'soundcloud': 'http://feeds.soundcloud.com/users/soundcloud:users:2640728/sounds.rss',
            'getoutspoken': 'https://getoutspoken.com/',
        }
        
        self.lgbtq_keywords = [
            'LGBTQ', 'LGBT', 'gay', 'lesbian', 'transgender', 'bisexual', 
            'queer', 'pride', 'rainbow', 'homosexual', 'same-sex', 
            'gender identity', 'sexual orientation', 'marriage equality',
            'trans rights', 'gay rights', 'GLAAD', 'Pride Month'
        ]
        
        # Default excluded keywords (can be overridden)
        self.default_excluded_keywords = [
            'death', 'died', 'suicide', 'murder', 'killed', 'violence',
            'attack', 'assault', 'harassment', 'abuse', 'hate crime'
        ]

    def normalize_datetime(self, dt: datetime) -> datetime:
        """Normalize datetime to timezone-aware UTC"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)

    def _fix_google_news_url(self, url: str) -> str:
        """Convert Google News web URLs to proper RSS format"""
        if 'news.google.com' not in url:
            return url
        
        # Fix search URLs
        if '/search?' in url and '/rss/' not in url:
            return url.replace('/search?', '/rss/search?')
        
        # Fix topic URLs  
        if '/topics/' in url and '/rss/' not in url:
            return url.replace('/topics/', '/rss/topics/')
        
        return url

    def _handle_google_news_feed(self, feed_url: str, source_name: str):
        """Special handling for Google News feeds with fallbacks"""
        try:
            # Fix the URL format first
            fixed_url = self._fix_google_news_url(feed_url)
            
            if fixed_url != feed_url:
                self.logger.info(f"Fixed Google News URL for {source_name}: {fixed_url}")
            
            # Try to fetch with specific headers that Google News likes
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0; +http://www.example.com/bot)',
                'Accept': 'application/rss+xml, application/xml, text/xml'
            }
            
            # Use requests to fetch with custom headers
            response = self.session.get(fixed_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # Parse the response
            feed = feedparser.parse(response.content)
            
            # Check if we got valid content
            if not feed.entries and hasattr(feed, 'bozo') and feed.bozo:
                self.logger.warning(f"Google News feed {source_name} returned no entries, trying alternative approach...")
                
                # Try the original URL as fallback
                fallback_feed = feedparser.parse(feed_url)
                if fallback_feed.entries:
                    return fallback_feed
                
                # If still no luck, skip this feed for this round
                self.logger.warning(f"Skipping {source_name} due to parsing issues")
                return feedparser.FeedParserDict()  # Return empty feed
            
            return feed
            
        except Exception as e:
            self.logger.error(f"Error handling Google News feed {source_name}: {e}")
            return feedparser.FeedParserDict()  # Return empty feed

    def _fetch_and_fix_feed(self, feed_url: str, source_name: str):
        """Enhanced feed fetcher with special handling for different sources"""
        try:
            # Special handling for Google News feeds
            if 'news.google.com' in feed_url:
                return self._handle_google_news_feed(feed_url, source_name)
            
            # Special handling for The Advocate feed (encoding issues)
            elif 'advocate.com' in feed_url:
                response = self.session.get(feed_url, timeout=15)
                response.raise_for_status()
                
                content = response.text
                content = content.replace('encoding="us-ascii"', 'encoding="utf-8"')
                content = content.replace("encoding='us-ascii'", "encoding='utf-8'")
                
                return feedparser.parse(content)
            
            # Standard handling for other feeds
            else:
                return feedparser.parse(feed_url)
                
        except Exception as e:
            self.logger.error(f"Error fetching feed {source_name}: {e}")
            return feedparser.FeedParserDict()  # Return empty feed

    def validate_feed_health(self) -> Dict[str, bool]:
        """Check which feeds are currently working"""
        feed_health = {}
        
        for source_name, feed_url in self.rss_feeds.items():
            try:
                self.logger.info(f"Testing feed: {source_name}")
                
                # Quick test fetch
                feed = self._fetch_and_fix_feed(feed_url, source_name)
                
                # Check if feed has entries and no major errors
                has_entries = bool(feed.entries)
                has_major_error = hasattr(feed, 'bozo') and feed.bozo and 'syntax error' in str(feed.bozo_exception)
                
                feed_health[source_name] = has_entries and not has_major_error
                
                if not feed_health[source_name]:
                    self.logger.warning(f"Feed {source_name} appears unhealthy: entries={has_entries}, error={has_major_error}")
                
            except Exception as e:
                self.logger.error(f"Feed {source_name} failed health check: {e}")
                feed_health[source_name] = False
        
        healthy_feeds = sum(feed_health.values())
        total_feeds = len(feed_health)
        self.logger.info(f"Feed health check complete: {healthy_feeds}/{total_feeds} feeds healthy")
        
        return feed_health

    def filter_articles_by_keywords(self, articles: List[Article], exclude_keywords: List[str] = None) -> List[Article]:
        """Filter out articles containing excluded keywords"""
        if not exclude_keywords:
            exclude_keywords = self.default_excluded_keywords
            
        if not exclude_keywords:
            return articles
        
        filtered_articles = []
        excluded_count = 0
        
        # Convert keywords to lowercase for case-insensitive matching
        exclude_keywords_lower = [kw.lower() for kw in exclude_keywords]
        
        for article in articles:
            # Check title and content for excluded keywords
            text_to_check = f"{article.title} {article.content}".lower()
            
            # Check if any excluded keyword appears in the article
            contains_excluded = any(keyword in text_to_check for keyword in exclude_keywords_lower)
            
            if not contains_excluded:
                filtered_articles.append(article)
            else:
                excluded_count += 1
                self.logger.debug(f"Excluded article: {article.title[:50]}...")
        
        if excluded_count > 0:
            self.logger.info(f"Filtered out {excluded_count} articles containing excluded keywords")
        
        return filtered_articles

    def _get_raw_content(self, entry) -> str:
        """Get raw HTML content without cleaning"""
        content_fields = ['content', 'summary', 'description']
        
        for field in content_fields:
            if hasattr(entry, field):
                content = getattr(entry, field)
                if isinstance(content, list) and content:
                    return content[0].get('value', '')
                elif isinstance(content, str):
                    return content
        
        return ""

    def extract_thumbnail_from_url(self, url: str, max_retries: int = 2) -> Optional[str]:
        """Extract thumbnail from article URL using Open Graph tags with improved error handling"""
        
        # Skip thumbnail extraction for problematic domains
        problematic_domains = [
            'thepinknews.com',  # Connection issues
            'soundcloud.com',   # Different content type
            'getoutspoken.com'  # May not have proper meta tags
        ]
        
        # Check if URL is from a problematic domain
        for domain in problematic_domains:
            if domain in url:
                self.logger.debug(f"Skipping thumbnail extraction for problematic domain: {domain}")
                return None
        
        # Domains known to work well with thumbnail extraction
        reliable_domains = [
            'advocate.com',
            'queerty.com', 
            'lgbtqnation.com',
            'washingtonblade.com',
            'theguardian.com',
            'them.us',
            'gaycitynews.com',
            'outsports.com'
        ]
        
        # Only extract from reliable domains to reduce errors
        is_reliable_domain = any(domain in url for domain in reliable_domains)
        if not is_reliable_domain:
            self.logger.debug(f"Skipping thumbnail extraction for non-whitelisted domain: {url}")
            return None
        
        for attempt in range(max_retries + 1):
            try:
                # Use more browser-like headers
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Cache-Control': 'max-age=0'
                }
                
                # Create a new session for thumbnail extraction with custom settings
                thumbnail_session = requests.Session()
                thumbnail_session.headers.update(headers)
                
                # Set shorter timeout and add retry strategy
                response = thumbnail_session.get(
                    url, 
                    timeout=(5, 10),  # (connection timeout, read timeout)
                    allow_redirects=True,
                    stream=False
                )
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Try multiple meta tag approaches in order of preference
                    meta_selectors = [
                        ('meta', {'property': 'og:image'}),
                        ('meta', {'property': 'og:image:url'}),
                        ('meta', {'name': 'twitter:image'}),
                        ('meta', {'name': 'twitter:image:src'}),
                        ('meta', {'property': 'twitter:image'}),
                        ('meta', {'name': 'msapplication-TileImage'}),
                        ('meta', {'itemprop': 'image'}),
                    ]
                    
                    for tag_name, attrs in meta_selectors:
                        meta_tag = soup.find(tag_name, attrs)
                        if meta_tag and meta_tag.get('content'):
                            img_url = meta_tag['content']
                            if img_url and not img_url.startswith('data:') and len(img_url) > 10:
                                # Resolve relative URLs
                                full_url = urljoin(url, img_url)
                                self.logger.debug(f"Found thumbnail for {url}: {full_url}")
                                return full_url
                    
                    # Try structured data as fallback
                    json_lds = soup.find_all('script', type='application/ld+json')
                    for script in json_lds[:3]:  # Limit to first 3 to avoid processing too much
                        try:
                            import json
                            data = json.loads(script.string)
                            if isinstance(data, dict) and 'image' in data:
                                img = data['image']
                                if isinstance(img, list) and img:
                                    return urljoin(url, img[0])
                                elif isinstance(img, str):
                                    return urljoin(url, img)
                        except (json.JSONDecodeError, ValueError):
                            continue
                    
                    self.logger.debug(f"No suitable thumbnail found in meta tags for: {url}")
                    return None
                else:
                    self.logger.debug(f"HTTP {response.status_code} when fetching thumbnail from: {url}")
                    return None
                    
            except (requests.exceptions.ConnectionError, 
                    requests.exceptions.Timeout,
                    requests.exceptions.RequestException) as e:
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 2  # Exponential backoff: 2s, 4s
                    self.logger.debug(f"Thumbnail extraction attempt {attempt + 1} failed for {url}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    self.logger.debug(f"Failed to extract thumbnail from {url} after {max_retries + 1} attempts: {type(e).__name__}")
                    return None
            except Exception as e:
                self.logger.debug(f"Unexpected error extracting thumbnail from {url}: {e}")
                return None
        
        return None

    def extract_thumbnail_from_rss(self, entry) -> Optional[str]:
        """Extract thumbnail image from RSS entry"""
        # Check for media:thumbnail or media:content
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url')
        
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image'):
                    return media.get('url')
        
        # Check for enclosure
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if enclosure.get('type', '').startswith('image'):
                    return enclosure.get('href')
        
        # Parse RAW content for images (before cleaning)
        raw_content = self._get_raw_content(entry)
        if raw_content:
            # Try multiple image extraction patterns
            patterns = [
                r'<img[^>]+src=[\'"]([^\'"]+)[\'"][^>]*>',
                r'<figure[^>]*>.*?<img[^>]+src=[\'"]([^\'"]+)[\'"].*?</figure>',
                r'background-image:\s*url\([\'"]?([^\'"]+)[\'"]?\)',
                r'data-src=[\'"]([^\'"]+)[\'"]',
                r'srcset=[\'"]([^\'"]+)[\'"]'
            ]
            
            for pattern in patterns:
                img_match = re.search(pattern, raw_content, re.DOTALL | re.IGNORECASE)
                if img_match:
                    url = img_match.group(1)
                    # Skip data URLs and very small images
                    if not url.startswith('data:') and 'pixel' not in url.lower():
                        return url
        
        return None

    def extract_thumbnail_from_url(self, url: str) -> Optional[str]:
        """Extract thumbnail from article URL using Open Graph tags"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Try multiple meta tag approaches
                meta_selectors = [
                    ('meta', {'property': 'og:image'}),
                    ('meta', {'property': 'og:image:url'}),
                    ('meta', {'name': 'twitter:image'}),
                    ('meta', {'name': 'twitter:image:src'}),
                    ('meta', {'property': 'twitter:image'}),
                    ('meta', {'name': 'msapplication-TileImage'}),
                    ('meta', {'itemprop': 'image'}),
                ]
                
                for tag_name, attrs in meta_selectors:
                    meta_tag = soup.find(tag_name, attrs)
                    if meta_tag and meta_tag.get('content'):
                        img_url = meta_tag['content']
                        if img_url and not img_url.startswith('data:'):
                            return urljoin(url, img_url)
                
                # Try structured data
                json_lds = soup.find_all('script', type='application/ld+json')
                for script in json_lds:
                    try:
                        import json
                        data = json.loads(script.string)
                        if isinstance(data, dict) and 'image' in data:
                            img = data['image']
                            if isinstance(img, list) and img:
                                return urljoin(url, img[0])
                            elif isinstance(img, str):
                                return urljoin(url, img)
                    except:
                        continue
                
                # Try article content images
                article_selectors = [
                    'article img',
                    '.entry-content img',
                    '.post-content img', 
                    '.article-body img',
                    'main img'
                ]
                
                for selector in article_selectors:
                    img_tag = soup.select_one(selector)
                    if img_tag and img_tag.get('src'):
                        src = img_tag['src']
                        if not src.startswith('data:') and 'pixel' not in src.lower():
                            return urljoin(url, src)
                    
        except Exception as e:
            self.logger.warning(f"Failed to extract thumbnail from {url}: {e}")
        
        return None

    def collect_from_rss_with_resilience(self, hours_back: int = 24) -> List[Article]:
        """Collect articles from RSS feeds with improved error handling"""
        articles = []
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
        failed_feeds = []
        successful_feeds = []
        
        for source_name, feed_url in self.rss_feeds.items():
            try:
                self.logger.info(f"Fetching RSS feed: {source_name}")
                
                # Use enhanced feed fetcher
                feed = self._fetch_and_fix_feed(feed_url, source_name)
                
                if not feed.entries:
                    failed_feeds.append(source_name)
                    self.logger.warning(f"No entries found for {source_name}")
                    continue
                
                entry_count = 0
                for entry in feed.entries:
                    try:
                        pub_date = self._parse_date(entry)
                        
                        if pub_date:
                            normalized_pub_date = self.normalize_datetime(pub_date)
                            if normalized_pub_date < cutoff_time:
                                continue
                        
                        content = self._extract_content(entry)
                        thumbnail_url = self.extract_thumbnail_from_rss(entry)
                        
                        # If no thumbnail from RSS, try extracting from URL (original behavior)
                        if not thumbnail_url:
                            thumbnail_url = self.extract_thumbnail_from_url(entry.link)
                        
                        final_pub_date = normalized_pub_date if pub_date else datetime.now(timezone.utc)
                        
                        article = Article(
                            title=entry.title,
                            content=content,
                            url=entry.link,
                            published_date=final_pub_date,
                            source=source_name,
                            thumbnail_url=thumbnail_url
                        )
                        
                        articles.append(article)
                        entry_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"Error processing entry from {source_name}: {e}")
                        continue
                
                if entry_count > 0:
                    successful_feeds.append(f"{source_name} ({entry_count} articles)")
                    self.logger.info(f"Successfully collected {entry_count} articles from {source_name}")
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                failed_feeds.append(source_name)
                self.logger.error(f"Failed to fetch RSS feed {source_name}: {e}")
                continue
        
        # Log summary
        self.logger.info(f"RSS collection complete: {len(successful_feeds)} successful, {len(failed_feeds)} failed")
        if successful_feeds:
            self.logger.info(f"Successful feeds: {', '.join(successful_feeds)}")
        if failed_feeds:
            self.logger.warning(f"Failed feeds: {', '.join(failed_feeds)}")
        
        return articles

    # Also update the collect_from_rss_with_resilience method to better handle thumbnail extraction:
    def collect_from_rss_with_resilience(self, hours_back: int = 24) -> List[Article]:
        """Collect articles from RSS feeds with improved error handling and thumbnail extraction"""
        articles = []
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
        failed_feeds = []
        successful_feeds = []
        
        for source_name, feed_url in self.rss_feeds.items():
            try:
                self.logger.info(f"Fetching RSS feed: {source_name}")
                
                # Use enhanced feed fetcher
                feed = self._fetch_and_fix_feed(feed_url, source_name)
                
                if not feed.entries:
                    failed_feeds.append(source_name)
                    self.logger.warning(f"No entries found for {source_name}")
                    continue
                
                entry_count = 0
                for entry in feed.entries:
                    try:
                        pub_date = self._parse_date(entry)
                        
                        if pub_date:
                            normalized_pub_date = self.normalize_datetime(pub_date)
                            if normalized_pub_date < cutoff_time:
                                continue
                        
                        content = self._extract_content(entry)
                        
                        # Try RSS thumbnail first (more reliable)
                        thumbnail_url = self.extract_thumbnail_from_rss(entry)
                        
                        # Only try URL extraction for high-priority sources if RSS didn't work
                        if not thumbnail_url and source_name in ['advocate', 'queerty', 'lgbtqnation', 'theguardian']:
                            thumbnail_url = self.extract_thumbnail_from_url(entry.link)
                        
                        final_pub_date = normalized_pub_date if pub_date else datetime.now(timezone.utc)
                        
                        article = Article(
                            title=entry.title,
                            content=content,
                            url=entry.link,
                            published_date=final_pub_date,
                            source=source_name,
                            thumbnail_url=thumbnail_url
                        )
                        
                        articles.append(article)
                        entry_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"Error processing entry from {source_name}: {e}")
                        continue
                
                if entry_count > 0:
                    successful_feeds.append(f"{source_name} ({entry_count} articles)")
                    self.logger.info(f"Successfully collected {entry_count} articles from {source_name}")
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                failed_feeds.append(source_name)
                self.logger.error(f"Failed to fetch RSS feed {source_name}: {e}")
                continue
        
        # Log summary
        self.logger.info(f"RSS collection complete: {len(successful_feeds)} successful, {len(failed_feeds)} failed")
        if successful_feeds:
            self.logger.info(f"Successful feeds: {', '.join(successful_feeds)}")
        if failed_feeds:
            self.logger.warning(f"Failed feeds: {', '.join(failed_feeds)}")
        
        return articles

    def collect_from_newsapi(self, hours_back: int = 24) -> List[Article]:
        """Collect articles from NewsAPI with thumbnail support"""
        if not self.newsapi_key:
            self.logger.warning("NewsAPI key not provided, skipping NewsAPI collection")
            return []
        
        articles = []
        from_date = (datetime.now(timezone.utc) - timedelta(hours=hours_back)).strftime('%Y-%m-%d')
        
        for keyword in self.lgbtq_keywords[:5]:
            try:
                url = "https://newsapi.org/v2/everything"
                params = {
                    'q': keyword,
                    'from': from_date,
                    'sortBy': 'publishedAt',
                    'language': 'en',
                    'apiKey': self.newsapi_key,
                    'pageSize': 20
                }
                
                response = self.session.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                for article_data in data.get('articles', []):
                    try:
                        if not article_data.get('content') or article_data['content'] == '[Removed]':
                            continue
                        
                        pub_date = datetime.fromisoformat(
                            article_data['publishedAt'].replace('Z', '+00:00')
                        )
                        
                        thumbnail_url = article_data.get('urlToImage')
                        
                        article = Article(
                            title=article_data['title'],
                            content=article_data['description'] or article_data['content'],
                            url=article_data['url'],
                            published_date=pub_date,
                            source=article_data['source']['name'],
                            thumbnail_url=thumbnail_url
                        )
                        
                        articles.append(article)
                        
                    except Exception as e:
                        self.logger.error(f"Error processing NewsAPI article: {e}")
                        continue
                
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f"Error fetching from NewsAPI with keyword '{keyword}': {e}")
                continue
        
        return articles

    def collect_all_sources(self, hours_back: int = 24, exclude_keywords: List[str] = None) -> List[Article]:
        """Collect from all available sources with improved resilience"""
        all_articles = []
        
        rss_articles = self.collect_from_rss_with_resilience(hours_back)
        all_articles.extend(rss_articles)
        self.logger.info(f"Collected {len(rss_articles)} articles from RSS feeds")
        
        news_articles = self.collect_from_newsapi(hours_back)
        all_articles.extend(news_articles)
        self.logger.info(f"Collected {len(news_articles)} articles from NewsAPI")
        
        unique_articles = self._deduplicate_articles(all_articles)
        self.logger.info(f"After deduplication: {len(unique_articles)} unique articles")
        
        filtered_articles = self.filter_articles_by_keywords(unique_articles, exclude_keywords)
        self.logger.info(f"After keyword filtering: {len(filtered_articles)} articles")
        
        return filtered_articles

    def generate_google_news_url(self, query: str, language: str = 'en-US', country: str = 'US') -> str:
        """Generate a proper Google News RSS URL from a search query"""
        encoded_query = urllib.parse.quote_plus(query)
        return f'https://news.google.com/rss/search?q={encoded_query}&hl={language}&gl={country}&ceid={country}%3A{language.split("-")[0]}'

    def _parse_date(self, entry) -> Optional[datetime]:
        """Parse date from RSS entry with timezone handling"""
        date_fields = ['published_parsed', 'updated_parsed']
        
        for field in date_fields:
            if hasattr(entry, field) and getattr(entry, field):
                try:
                    time_struct = getattr(entry, field)
                    # Create timezone-aware datetime (assume UTC for RSS feeds)
                    dt = datetime(*time_struct[:6], tzinfo=timezone.utc)
                    return dt
                except:
                    continue
        
        date_strings = ['published', 'updated']
        for field in date_strings:
            if hasattr(entry, field):
                try:
                    from dateutil import parser
                    # Handle timezone parsing with EDT/EST mapping
                    tzinfos = {
                        'EDT': timezone(timedelta(hours=-4)),  # Eastern Daylight Time
                        'EST': timezone(timedelta(hours=-5)),  # Eastern Standard Time
                        'PDT': timezone(timedelta(hours=-7)),  # Pacific Daylight Time
                        'PST': timezone(timedelta(hours=-8)),  # Pacific Standard Time
                        'CDT': timezone(timedelta(hours=-5)),  # Central Daylight Time
                        'CST': timezone(timedelta(hours=-6)),  # Central Standard Time
                        'MDT': timezone(timedelta(hours=-6)),  # Mountain Daylight Time
                        'MST': timezone(timedelta(hours=-7)),  # Mountain Standard Time
                    }
                    dt = parser.parse(getattr(entry, field), tzinfos=tzinfos)
                    # Ensure timezone-aware
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except:
                    continue
        
        return None

    def _extract_content(self, entry) -> str:
        """Extract and clean content from RSS entry"""
        content_fields = ['content', 'summary', 'description']
        
        for field in content_fields:
            if hasattr(entry, field):
                content = getattr(entry, field)
                if isinstance(content, list) and content:
                    raw_content = content[0].get('value', '')
                elif isinstance(content, str):
                    raw_content = content
                else:
                    continue
                
                # Clean HTML tags and return clean text
                return self._clean_html_content(raw_content)
        
        return ""

    def _clean_html_content(self, html_content: str) -> str:
        """Remove HTML tags and clean up content"""
        if not html_content:
            return ""
        
        # Parse HTML and extract text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        
        # Clean up whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text

    def _deduplicate_articles(self, articles: List[Article]) -> List[Article]:
        """Remove duplicate articles based on hash_id"""
        seen_hashes = set()
        unique_articles = []
        
        for article in articles:
            if article.hash_id not in seen_hashes:
                seen_hashes.add(article.hash_id)
                unique_articles.append(article)
        
        return unique_articles

# Flask App
app = Flask(__name__)
collector = LGBTQNewsCollector()

# Replace your index route with this updated version that includes inline HTML template:

@app.route('/')
def index():
    """Main page displaying recent LGBTQ+ news articles with aligned form"""
    hours_back = request.args.get('hours', 24, type=int)
    exclude_param = request.args.get('exclude', '')
    
    # Parse exclude keywords from comma-separated string
    exclude_keywords = [kw.strip() for kw in exclude_param.split(',') if kw.strip()] if exclude_param else None
    
    articles = collector.collect_all_sources(hours_back=hours_back, exclude_keywords=exclude_keywords)
    
    # Sort by publication date (newest first) with normalized datetimes
    articles.sort(key=lambda x: collector.normalize_datetime(x.published_date), reverse=True)
    
    # Create inline HTML template with aligned form
    articles_html = ""
    for article in articles:
        thumbnail_html = ""
        if article.thumbnail_url:
            thumbnail_html = f'<img src="{article.thumbnail_url}" alt="Article thumbnail" style="width: 100px; height: 60px; object-fit: cover; border-radius: 5px; margin-right: 15px;">'
        
        articles_html += f'''
        <div style="display: flex; padding: 20px; margin: 15px 0; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); border-left: 4px solid #007bff;">
            {thumbnail_html}
            <div style="flex: 1;">
                <h3 style="margin: 0 0 8px 0; color: #495057;">
                    <a href="{article.url}" target="_blank" style="text-decoration: none; color: #007bff;">{article.title}</a>
                </h3>
                <p style="margin: 8px 0; color: #6c757d; line-height: 1.4;">{article.content[:200]}{'...' if len(article.content) > 200 else ''}</p>
                <div style="font-size: 12px; color: #868e96; margin-top: 10px;">
                    <span style="background: #e9ecef; padding: 2px 8px; border-radius: 12px; margin-right: 10px;">{article.source}</span>
                    <span>{article.published_date.strftime('%Y-%m-%d %H:%M UTC')}</span>
                </div>
            </div>
        </div>
        '''
    
    html_template = f'''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>🧠 The Brain - LGBTQ+ News Aggregator</title>
        <style>
            * {{
                box-sizing: border-box;
            }}
            body {{
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                margin: 0;
                padding: 20px;
                min-height: 100vh;
            }}
            .container {{
                max-width: 1000px;
                margin: 0 auto;
                background: #f8f9fa;
                border-radius: 15px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.2);
                overflow: hidden;
            }}
            .header {{
                background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
                padding: 30px;
                text-align: center;
                color: white;
            }}
            .header h1 {{
                margin: 0;
                font-size: 2.5em;
                font-weight: 300;
                text-shadow: 0 2px 4px rgba(0,0,0,0.3);
            }}
            .header p {{
                margin: 10px 0 0 0;
                opacity: 0.9;
                font-size: 1.1em;
            }}
            .controls {{
                padding: 25px;
                background: white;
                border-bottom: 1px solid #dee2e6;
            }}
            .form-row {{
                display: grid;
                grid-template-columns: 200px 1fr;
                gap: 15px;
                align-items: center;
                margin-bottom: 15px;
            }}
            .form-row:last-child {{
                margin-bottom: 0;
            }}
            .form-label {{
                font-weight: 600;
                color: #495057;
                text-align: right;
                padding-right: 10px;
            }}
            .form-input {{
                display: flex;
                gap: 10px;
                align-items: center;
            }}
            input[type="number"], input[type="text"] {{
                padding: 8px 12px;
                border: 2px solid #e9ecef;
                border-radius: 6px;
                font-size: 14px;
                transition: border-color 0.3s ease;
                flex: 1;
            }}
            input[type="number"]:focus, input[type="text"]:focus {{
                outline: none;
                border-color: #007bff;
                box-shadow: 0 0 0 3px rgba(0,123,255,0.1);
            }}
            .btn {{
                background: #007bff;
                color: white;
                border: none;
                padding: 8px 20px;
                border-radius: 6px;
                cursor: pointer;
                font-size: 14px;
                font-weight: 500;
                transition: background-color 0.3s ease;
                text-decoration: none;
                display: inline-block;
            }}
            .btn:hover {{
                background: #0056b3;
            }}
            .btn-secondary {{
                background: #6c757d;
                margin-left: 10px;
            }}
            .btn-secondary:hover {{
                background: #545b62;
            }}
            .stats {{
                padding: 20px 25px;
                background: #e8f4fd;
                border-bottom: 1px solid #bee5eb;
                text-align: center;
            }}
            .stats-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                gap: 20px;
                margin-top: 15px;
            }}
            .stat-item {{
                background: white;
                padding: 15px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            .stat-number {{
                font-size: 2em;
                font-weight: bold;
                color: #007bff;
                margin: 0;
            }}
            .stat-label {{
                font-size: 0.9em;
                color: #6c757d;
                margin: 5px 0 0 0;
            }}
            .articles {{
                padding: 25px;
                background: #f8f9fa;
            }}
            .no-articles {{
                text-align: center;
                padding: 40px;
                color: #6c757d;
            }}
            .form-help {{
                font-size: 12px;
                color: #6c757d;
                margin-top: 5px;
            }}
            @media (max-width: 768px) {{
                .form-row {{
                    grid-template-columns: 1fr;
                    text-align: left;
                }}
                .form-label {{
                    text-align: left;
                    padding-right: 0;
                    margin-bottom: 5px;
                }}
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>🧠 The Brain</h1>
                <p>Your LGBTQ+ News Intelligence Hub</p>
            </div>
            
            <form method="GET" class="controls">
                <div class="form-row">
                    <label class="form-label">Hours back:</label>
                    <div class="form-input">
                        <input type="number" name="hours" value="{hours_back}" min="1" max="168" style="width: 100px;">
                        <button type="submit" class="btn">Update</button>
                        <a href="/refresh" class="btn btn-secondary">Refresh</a>
                        <a href="/admin" class="btn" style="background: #28a745;">Admin</a>
                    </div>
                </div>
                
                <div class="form-row">
                    <label class="form-label">Exclude keywords<br><span style="font-size: 11px; font-weight: normal;">(comma-separated):</span></label>
                    <div class="form-input">
                        <input type="text" name="exclude" value="{exclude_param}" 
                               placeholder="e.g., death, violence, murder">
                        <div class="form-help">
                            <strong>Default excluded:</strong> {','.join(collector.default_excluded_keywords)}<br>
                            <em>Leave blank to use defaults, or add your own terms</em>
                        </div>
                    </div>
                </div>
            </form>
            
            <div class="stats">
                <h3 style="margin: 0; color: #495057;">📊 Current Collection Stats</h3>
                <div class="stats-grid">
                    <div class="stat-item">
                        <div class="stat-number">{len(articles)}</div>
                        <div class="stat-label">Articles Found</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{hours_back}</div>
                        <div class="stat-label">Hours Coverage</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{len(set(article.source for article in articles))}</div>
                        <div class="stat-label">News Sources</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">{len([a for a in articles if a.thumbnail_url])}</div>
                        <div class="stat-label">With Images</div>
                    </div>
                </div>
            </div>
            
            <div class="articles">
                {"<div class='no-articles'><h3>📰 No articles found</h3><p>Try increasing the hours back or adjusting your keyword filters.</p></div>" if not articles else articles_html}
            </div>
        </div>
    </body>
    </html>
    '''
    
    return html_template

@app.route('/api/articles')
def api_articles():
    """API endpoint to get articles in JSON format"""
    hours_back = request.args.get('hours', 24, type=int)
    exclude_param = request.args.get('exclude', '')
    
    # Parse exclude keywords from comma-separated string
    exclude_keywords = [kw.strip() for kw in exclude_param.split(',') if kw.strip()] if exclude_param else None
    
    articles = collector.collect_all_sources(hours_back=hours_back, exclude_keywords=exclude_keywords)
    
    articles_data = []
    for article in articles:
        articles_data.append({
            'title': article.title,
            'content': article.content,
            'url': article.url,
            'published_date': article.published_date.isoformat(),
            'source': article.source,
            'thumbnail_url': article.thumbnail_url,
            'hash_id': article.hash_id
        })
    
    return jsonify({
        'articles': articles_data,
        'count': len(articles_data),
        'excluded_keywords': exclude_keywords or []
    })

@app.route('/refresh')
def refresh():
    """Force refresh of articles"""
    return redirect('/')

# Replace your admin dashboard route with this improved version:

@app.route('/admin')
def admin_dashboard():
    """Simple admin dashboard with improved JavaScript and error handling"""
    return '''
    <!DOCTYPE html>
    <html>
    <head>
        <title>The Brain - Admin Dashboard</title>
        <style>
            body { 
                font-family: Arial, sans-serif; 
                margin: 40px; 
                background-color: #f5f5f5; 
            }
            .container { 
                max-width: 1200px; 
                margin: 0 auto; 
                background: white; 
                padding: 30px; 
                border-radius: 10px; 
                box-shadow: 0 2px 10px rgba(0,0,0,0.1); 
            }
            .status-healthy { color: #28a745; font-weight: bold; }
            .status-unhealthy { color: #dc3545; font-weight: bold; }
            button { 
                padding: 12px 20px; 
                margin: 8px; 
                background: #007bff;
                color: white;
                border: none;
                border-radius: 5px;
                cursor: pointer;
                font-size: 14px;
                transition: background-color 0.3s;
            }
            button:hover { background: #0056b3; }
            button:disabled { 
                background: #6c757d; 
                cursor: not-allowed; 
            }
            .results { 
                margin-top: 20px; 
                padding: 20px; 
                background: #f8f9fa; 
                border-radius: 5px; 
                border-left: 4px solid #007bff; 
                min-height: 50px;
            }
            .feed-list { list-style: none; padding: 0; }
            .feed-item { 
                background: white; 
                margin: 10px 0; 
                padding: 15px; 
                border-radius: 5px; 
                border: 1px solid #dee2e6; 
            }
            .feed-url { color: #6c757d; font-size: 12px; word-break: break-all; }
            .header { color: #495057; border-bottom: 2px solid #007bff; padding-bottom: 10px; }
            .error { 
                background: #f8d7da; 
                color: #721c24; 
                padding: 15px; 
                border-radius: 5px; 
                margin: 10px 0;
                border-left: 4px solid #dc3545;
            }
            .loading {
                background: #d4edda;
                color: #155724;
                padding: 15px;
                border-radius: 5px;
                margin: 10px 0;
                border-left: 4px solid #28a745;
            }
            .debug {
                background: #e2e3e5;
                color: #383d41;
                padding: 10px;
                border-radius: 3px;
                font-family: monospace;
                font-size: 12px;
                margin-top: 10px;
                max-height: 200px;
                overflow-y: auto;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1 class="header">🧠 The Brain - Feed Administration</h1>
            
            <h2>Feed Health Monitoring</h2>
            <p>Monitor and test your LGBTQ+ news feed sources for "The Brain" project.</p>
            
            <div style="margin: 20px 0;">
                <button id="healthBtn" onclick="checkFeedHealth()">🔍 Check All Feeds Health</button>
                <button id="listBtn" onclick="listFeeds()">📝 List All Feeds</button>
                <button id="statsBtn" onclick="showStats()">📊 Show Statistics</button>
                <button onclick="goHome()" style="background: #28a745;">🏠 Back to News</button>
                <button onclick="toggleDebug()" style="background: #6c757d;" id="debugBtn">🐛 Debug</button>
            </div>
            
            <div id="results" class="results" style="display: none;">
                <div id="content">Ready to check feeds...</div>
                <div id="debug" class="debug" style="display: none;">
                    <strong>Debug Log:</strong><br>
                    <div id="debugContent">Click 'Debug' to show debug information</div>
                </div>
            </div>
        </div>
        
        <script>
            let debugMode = false;
            let debugLog = [];
            
            function log(message) {
                console.log(message);
                debugLog.push(new Date().toLocaleTimeString() + ': ' + message);
                if (debugMode) {
                    updateDebugDisplay();
                }
            }
            
            function updateDebugDisplay() {
                const debugContent = document.getElementById('debugContent');
                if (debugContent) {
                    debugContent.innerHTML = debugLog.slice(-20).join('<br>'); // Show last 20 entries
                }
            }
            
            function toggleDebug() {
                debugMode = !debugMode;
                const debugDiv = document.getElementById('debug');
                const debugBtn = document.getElementById('debugBtn');
                
                if (debugMode) {
                    debugDiv.style.display = 'block';
                    debugBtn.textContent = '🐛 Hide Debug';
                    updateDebugDisplay();
                } else {
                    debugDiv.style.display = 'none';
                    debugBtn.textContent = '🐛 Debug';
                }
                log('Debug mode ' + (debugMode ? 'enabled' : 'disabled'));
            }
            
            function showResults() {
                document.getElementById('results').style.display = 'block';
            }
            
            function setLoading(message) {
                showResults();
                document.getElementById('content').innerHTML = 
                    '<div class="loading">🔄 ' + message + '</div>';
                log('Loading: ' + message);
            }
            
            function setError(message, details = '') {
                showResults();
                document.getElementById('content').innerHTML = 
                    '<div class="error">❌ ' + message + 
                    (details ? '<br><small>' + details + '</small>' : '') + '</div>';
                log('Error: ' + message + (details ? ' - ' + details : ''));
            }
            
            function disableButton(buttonId) {
                const btn = document.getElementById(buttonId);
                if (btn) {
                    btn.disabled = true;
                    setTimeout(() => { btn.disabled = false; }, 3000);
                }
            }
            
            function goHome() {
                log('Navigating to home page');
                window.location.href = '/';
            }
            
            function checkFeedHealth() {
                log('Starting feed health check');
                setLoading('Checking feed health... This may take 30-60 seconds...');
                disableButton('healthBtn');
                
                fetch('/admin/feed-health')
                    .then(response => {
                        log('Feed health response status: ' + response.status);
                        if (!response.ok) {
                            throw new Error('HTTP ' + response.status + ': ' + response.statusText);
                        }
                        return response.json();
                    })
                    .then(data => {
                        log('Feed health data received: ' + JSON.stringify(data, null, 2));
                        
                        let healthyList = data.healthy_feed_list && data.healthy_feed_list.length > 0 
                            ? data.healthy_feed_list.join(', ') 
                            : 'None';
                        let unhealthyList = data.unhealthy_feed_list && data.unhealthy_feed_list.length > 0 
                            ? data.unhealthy_feed_list.join(', ') 
                            : 'None';
                            
                        document.getElementById('content').innerHTML = `
                            <h3>📊 Feed Health Report</h3>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                                <div style="text-align: center; padding: 20px; background: #d4edda; border-radius: 10px;">
                                    <h4 style="margin: 0; color: #155724;">✅ Healthy Feeds</h4>
                                    <div style="font-size: 2em; color: #28a745; margin: 10px 0;">${data.healthy_feeds || 0}</div>
                                </div>
                                <div style="text-align: center; padding: 20px; background: #f8d7da; border-radius: 10px;">
                                    <h4 style="margin: 0; color: #721c24;">❌ Unhealthy Feeds</h4>
                                    <div style="font-size: 2em; color: #dc3545; margin: 10px 0;">${data.unhealthy_feeds || 0}</div>
                                </div>
                            </div>
                            <p><strong>Total Feeds:</strong> ${data.total_feeds || 'Unknown'}</p>
                            <p><strong>Healthy Feeds:</strong> <span class="status-healthy">${healthyList}</span></p>
                            <p><strong>Unhealthy Feeds:</strong> <span class="status-unhealthy">${unhealthyList}</span></p>
                            <p><small>Last checked: ${data.timestamp ? new Date(data.timestamp).toLocaleString() : 'Unknown'}</small></p>
                        `;
                        log('Feed health check completed successfully');
                    })
                    .catch(error => {
                        setError('Failed to check feed health', error.message);
                        log('Feed health check failed: ' + error.message);
                    });
            }
            
            function listFeeds() {
                log('Starting feed list');
                setLoading('Loading feed list...');
                disableButton('listBtn');
                
                fetch('/admin/feeds')
                    .then(response => {
                        log('Feed list response status: ' + response.status);
                        if (!response.ok) {
                            throw new Error('HTTP ' + response.status + ': ' + response.statusText);
                        }
                        return response.json();
                    })
                    .then(data => {
                        log('Feed list data received: ' + JSON.stringify(data, null, 2));
                        
                        let html = '<h3>📰 All Configured News Feeds (' + (data.feeds ? data.feeds.length : 0) + ')</h3>';
                        
                        if (!data.feeds || data.feeds.length === 0) {
                            html += '<p>No feeds configured.</p>';
                        } else {
                            html += '<ul class="feed-list">';
                            data.feeds.forEach(feed => {
                                let typeIcon = feed.type === 'google_news' ? '🔍' : '📡';
                                html += `
                                    <li class="feed-item">
                                        <div style="display: flex; justify-content: space-between; align-items: center;">
                                            <div>
                                                <strong>${typeIcon} ${feed.name}</strong> 
                                                <span style="background: #e9ecef; padding: 2px 8px; border-radius: 12px; font-size: 11px; margin-left: 10px;">${feed.type}</span>
                                                <div class="feed-url">${feed.url}</div>
                                            </div>
                                            <button onclick="testFeed('${feed.name}')" style="margin: 0;">🧪 Test</button>
                                        </div>
                                    </li>
                                `;
                            });
                            html += '</ul>';
                        }
                        
                        document.getElementById('content').innerHTML = html;
                        log('Feed list loaded successfully');
                    })
                    .catch(error => {
                        setError('Failed to load feed list', error.message);
                        log('Feed list failed: ' + error.message);
                    });
            }
            
            function testFeed(feedName) {
                log('Testing feed: ' + feedName);
                if (!feedName) {
                    setError('Invalid feed name');
                    return;
                }
                
                if (confirm('Test feed: ' + feedName + '? This may take 10-30 seconds.')) {
                    setLoading('Testing feed: ' + feedName + '...');
                    
                    fetch('/admin/test-feed/' + encodeURIComponent(feedName))
                        .then(response => {
                            log('Test feed response status for ' + feedName + ': ' + response.status);
                            return response.json();
                        })
                        .then(data => {
                            log('Test feed data for ' + feedName + ': ' + JSON.stringify(data, null, 2));
                            
                            let status = data.error ? '❌ FAILED' : '✅ SUCCESS';
                            let details = data.error 
                                ? `Error: ${data.error}`
                                : `Found ${data.entries_found || 0} entries\\nFetch time: ${data.fetch_time_seconds || 'N/A'}s\\nFeed title: ${data.feed_title || 'Unknown'}`;
                            
                            alert(status + '\\n\\nFeed: ' + feedName + '\\n\\n' + details);
                            log('Test feed completed for ' + feedName);
                        })
                        .catch(error => {
                            setError('Test failed for feed: ' + feedName, error.message);
                            log('Test feed failed for ' + feedName + ': ' + error.message);
                        });
                }
            }
            
            function showStats() {
                log('Showing statistics');
                showResults();
                
                const stats = {
                    project: 'LGBTQ+ News Aggregator',
                    status: 'Development/Testing Phase',
                    sources: 'Multiple RSS feeds + NewsAPI',
                    keywords: 'LGBTQ+ focused terms',
                    excluded: 'Negative keywords filtered',
                    features: 'Thumbnail extraction, deduplication, timezone handling'
                };
                
                document.getElementById('content').innerHTML = `
                    <h3>📈 The Brain Statistics</h3>
                    <div style="display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: center;">
                        <div><strong>🎯 Project:</strong></div><div>${stats.project}</div>
                        <div><strong>🔧 Status:</strong></div><div>${stats.status}</div>
                        <div><strong>📡 Sources:</strong></div><div>${stats.sources}</div>
                        <div><strong>🏷️ Keywords:</strong></div><div>${stats.keywords}</div>
                        <div><strong>🚫 Excluded:</strong></div><div>${stats.excluded}</div>
                        <div><strong>⚡ Features:</strong></div><div>${stats.features}</div>
                    </div>
                    <div style="margin-top: 20px; padding: 15px; background: #e8f4fd; border-radius: 5px;">
                        <strong>🔗 Quick Links:</strong><br>
                        <a href="/" style="color: #007bff; text-decoration: none; margin-right: 20px;">📰 Main News Page</a>
                        <a href="/api/articles" style="color: #007bff; text-decoration: none; margin-right: 20px;">📋 API Endpoint</a>
                        <a href="/admin/feed-health" style="color: #007bff; text-decoration: none;">🔧 Feed Health JSON</a>
                    </div>
                `;
                log('Statistics displayed');
            }
            
            // Initialize page
            document.addEventListener('DOMContentLoaded', function() {
                log('Admin dashboard loaded');
                log('JavaScript initialized successfully');
            });
            
            // Test basic functionality on load
            log('Admin dashboard script loaded successfully');
        </script>
    </body>
    </html>
    '''

@app.route('/admin/feed-health')
def admin_feed_health():
    """API endpoint to check feed health status"""
    feed_health = collector.validate_feed_health()
    
    healthy_feeds = [name for name, status in feed_health.items() if status]
    unhealthy_feeds = [name for name, status in feed_health.items() if not status]
    
    return jsonify({
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'total_feeds': len(feed_health),
        'healthy_feeds': len(healthy_feeds),
        'unhealthy_feeds': len(unhealthy_feeds),
        'healthy_feed_list': healthy_feeds,
        'unhealthy_feed_list': unhealthy_feeds,
        'feed_details': feed_health
    })

@app.route('/admin/feeds')
def admin_list_feeds():
    """API endpoint to list all configured feeds"""
    return jsonify({
        'feeds': [
            {
                'name': name,
                'url': url,
                'type': 'google_news' if 'news.google.com' in url else 'standard_rss'
            }
            for name, url in collector.rss_feeds.items()
        ]
    })

@app.route('/admin/test-feed/<feed_name>')
def admin_test_single_feed(feed_name):
    """Test a specific feed and return detailed results"""
    if feed_name not in collector.rss_feeds:
        return jsonify({'error': 'Feed not found'}), 404
    
    feed_url = collector.rss_feeds[feed_name]
    
    try:
        start_time = time.time()
        
        feed = collector._fetch_and_fix_feed(feed_url, feed_name)
        
        fetch_time = time.time() - start_time
        
        return jsonify({
            'feed_name': feed_name,
            'feed_url': feed_url,
            'fetch_time_seconds': round(fetch_time, 2),
            'entries_found': len(feed.entries) if feed.entries else 0,
            'feed_title': getattr(feed.feed, 'title', 'Unknown') if hasattr(feed, 'feed') else 'Unknown',
            'last_updated': getattr(feed.feed, 'updated', 'Unknown') if hasattr(feed, 'feed') else 'Unknown',
            'has_bozo_error': hasattr(feed, 'bozo') and feed.bozo,
            'bozo_exception': str(feed.bozo_exception) if hasattr(feed, 'bozo_exception') else None,
            'sample_titles': [entry.title for entry in feed.entries[:3]] if feed.entries else []
        })
        
    except Exception as e:
        return jsonify({
            'feed_name': feed_name,
            'feed_url': feed_url,
            'error': str(e),
            'status': 'failed'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)